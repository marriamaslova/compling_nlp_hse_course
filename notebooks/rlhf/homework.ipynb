{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marriamaslova/compling_nlp_hse_course/blob/master/notebooks/rlhf/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec9e6bbc",
      "metadata": {
        "id": "ec9e6bbc"
      },
      "source": [
        "# Задание 1 (10 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4a0c17",
      "metadata": {
        "id": "ae4a0c17"
      },
      "source": [
        "Вам нужно воспроизвести пайплайн дообучения из семинара, используя русскоязычные модели. То есть, вам нужно дообучить генеративную модель генерировать более положительные тексты с помощью модели определения тональности.\n",
        "\n",
        "1. Вам потребуется модель определения тональности для русского языка. Вы можете найти ее на huggingface или обучить самостоятельно. В прошлых семинарах мы пользовались датасетом с токсичными текстами - можно использовать его для обучения (или любой другой датасет на русском языке). Удобнее всего будет обучать модель через huggingface. Можете взять за основу вот этот туториал - https://huggingface.co/docs/transformers/training#train-a-tensorflow-model-with-keras После обучения можете сохранить модель через model.save_pretrained(path) и потом использовать ее как любую другую модель из huggingface hub. \n",
        "Используйте не очень большую модель, чтобы все поместилось в колабе (например, distilbert-multilingual)\n",
        "\n",
        "2. Генеративную модель в целом можно взять любую, но лучше всего будет модель, специально обученная на русском языке (rugpt). "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Изначальная генерация текстов"
      ],
      "metadata": {
        "id": "vaFLUC-qh_nr"
      },
      "id": "vaFLUC-qh_nr"
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers \n",
        "! pip install wandb"
      ],
      "metadata": {
        "id": "DV7vlIcR5dT1"
      },
      "id": "DV7vlIcR5dT1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install datasets\n",
        "! pip install trl"
      ],
      "metadata": {
        "id": "9C5G93Kv5txv"
      },
      "id": "9C5G93Kv5txv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead\n",
        "from trl.core import LengthSampler"
      ],
      "metadata": {
        "id": "3uncwGUVgGum"
      },
      "id": "3uncwGUVgGum",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = PPOConfig(\n",
        "    model_name=\"ai-forever/rugpt3small_based_on_gpt2\",\n",
        "    learning_rate=1.41e-5,\n",
        "    log_with=None,\n",
        "    mini_batch_size=16\n",
        ")\n",
        "\n",
        "sent_kwargs = {\"return_all_scores\": True, \"function_to_apply\": \"none\", \"batch_size\": 16}"
      ],
      "metadata": {
        "id": "Mz88hIuCgGp1"
      },
      "id": "Mz88hIuCgGp1",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"csv\", data_files=\"/content/labeled.csv\")"
      ],
      "metadata": {
        "id": "b7jEyfMfd822"
      },
      "id": "b7jEyfMfd822",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(config, dataset=dataset['train'], input_min_text_length=2, input_max_text_length=8):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    ds = dataset\n",
        "    ds = ds.rename_columns({\"comment\": \"review\"})\n",
        "    \n",
        "    # еще есть фильтрация по длине\n",
        "    ds = ds.filter(lambda x: len(x[\"review\"]) > 200, batched=False)\n",
        "    ds = ds.filter(lambda x: len(x[\"review\"]) < 2000, batched=False)\n",
        "    \n",
        "    # длина кусочка определяется случайно\n",
        "    input_size = LengthSampler(input_min_text_length, input_max_text_length)\n",
        "\n",
        "    def tokenize(sample):\n",
        "        sample[\"input_ids\"] = tokenizer.encode(sample[\"review\"])[: input_size()]\n",
        "        sample[\"query\"] = tokenizer.decode(sample[\"input_ids\"])\n",
        "        return sample\n",
        "\n",
        "    ds = ds.map(tokenize, batched=False)\n",
        "    ds.set_format(type=\"torch\")\n",
        "    return ds"
      ],
      "metadata": {
        "id": "eefdTt9XgGnb"
      },
      "id": "eefdTt9XgGnb",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = build_dataset(config)\n",
        "\n",
        "def collator(data):\n",
        "    return dict((key, [d[key] for d in data]) for key in data[0])"
      ],
      "metadata": {
        "id": "8fD1qN4YgGlE"
      },
      "id": "8fD1qN4YgGlE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# active_model\n",
        "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "# reference_model (обратите внимание что это одна и так же модель изначально)\n",
        "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "WDlBtcASgGiZ"
      },
      "id": "WDlBtcASgGiZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_trainer = PPOTrainer(config, model, ref_model, tokenizer, dataset=dataset, data_collator=collator)"
      ],
      "metadata": {
        "id": "6yWQ_Lk0htZ5"
      },
      "id": "6yWQ_Lk0htZ5",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = ppo_trainer.accelerator.device\n",
        "if ppo_trainer.accelerator.num_processes == 1:\n",
        "    device = 0 if torch.cuda.is_available() else \"cpu\"  # to avoid a `pipeline` bug"
      ],
      "metadata": {
        "id": "kWuujEjehtXJ"
      },
      "id": "kWuujEjehtXJ",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовка модели для sentiment analysis\n",
        "\n",
        "Я очень долго пыталась обучить модель на собственном датасете, но как будто какой-то пример все время подавался неправильно и обучение не получалось (хотя я и размер примеров ограничивала, и многие параметры пробовала менять). В конце концов я сдалась и взяла предобученную модель."
      ],
      "metadata": {
        "id": "g7mBbKtWiEbc"
      },
      "id": "g7mBbKtWiEbc"
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipe = pipeline(\"sentiment-analysis\", model=\"cointegrated/rubert-tiny-sentiment-balanced\", device=device)"
      ],
      "metadata": {
        "id": "bN0rR0IMfLIb"
      },
      "id": "bN0rR0IMfLIb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"фильм отстой!\"\n",
        "sentiment_pipe(text, **sent_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYiHGEvbff3C",
        "outputId": "96dce9db-d8ca-41d5-8e13-c51a774c9a35"
      },
      "id": "HYiHGEvbff3C",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar funcionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'negative', 'score': 1.3824427127838135},\n",
              "  {'label': 'neutral', 'score': -0.7409297823905945},\n",
              "  {'label': 'positive', 'score': -0.6284522414207458}]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"лучший фильм на этой планете!!!\"\n",
        "sentiment_pipe(text, **sent_kwargs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqt8xPgffkXi",
        "outputId": "515e02e8-2e24-41ac-89ad-a1b0e58b91bd"
      },
      "id": "Pqt8xPgffkXi",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[{'label': 'negative', 'score': -3.0470917224884033},\n",
              "  {'label': 'neutral', 'score': -0.6917519569396973},\n",
              "  {'label': 'positive', 'score': 3.3792996406555176}]]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_kwargs = {\"min_length\": -1, \"top_k\": 0.0, \"top_p\": 1.0, \n",
        "              \"do_sample\": True, \"pad_token_id\": tokenizer.eos_token_id}"
      ],
      "metadata": {
        "id": "zikBBYEjft-g"
      },
      "id": "zikBBYEjft-g",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение генеративной модели"
      ],
      "metadata": {
        "id": "sb118jfxzhAX"
      },
      "id": "sb118jfxzhAX"
    },
    {
      "cell_type": "code",
      "source": [
        "output_min_length = 8\n",
        "output_max_length = 32\n",
        "output_length_sampler = LengthSampler(output_min_length, output_max_length)\n",
        "\n",
        "\n",
        "generation_kwargs = {\n",
        "    \"min_length\": -1,\n",
        "    \"top_k\": 0.0,\n",
        "    \"top_p\": 1.0,\n",
        "    \"do_sample\": True,\n",
        "    \"pad_token_id\": tokenizer.eos_token_id,\n",
        "}\n",
        "\n",
        "\n",
        "for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader), \n",
        "                         total=dataset.num_rows//ppo_trainer.dataloader.batch_sampler.batch_size):\n",
        "    query_tensors = batch[\"input_ids\"]\n",
        "\n",
        "    #### Get response from gpt2\n",
        "    response_tensors = []\n",
        "    for query in query_tensors:\n",
        "        gen_len = output_length_sampler()\n",
        "        generation_kwargs[\"max_new_tokens\"] = gen_len\n",
        "        response = ppo_trainer.generate(query, **generation_kwargs)\n",
        "        response_tensors.append(response.squeeze()[-gen_len:])\n",
        "    batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
        "\n",
        "    #### Compute sentiment score\n",
        "    texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
        "    pipe_outputs = sentiment_pipe(texts, **sent_kwargs)\n",
        "    rewards = [torch.tensor(output[2][\"score\"]) for output in pipe_outputs]\n",
        "\n",
        "    #### Run PPO step\n",
        "    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
        "    ppo_trainer.log_stats(stats, batch, rewards)"
      ],
      "metadata": {
        "id": "0V4VVqJOchs8"
      },
      "id": "0V4VVqJOchs8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 3000)\n",
        "pd.set_option('display.max_colwidth', 5000)"
      ],
      "metadata": {
        "id": "ryzMEhHwchqN"
      },
      "id": "ryzMEhHwchqN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### get a batch from the dataset\n",
        "bs = 32\n",
        "game_data = dict()\n",
        "dataset.set_format(\"pandas\")\n",
        "df_batch = dataset[:].sample(bs)\n",
        "game_data[\"query\"] = df_batch[\"query\"].tolist()\n",
        "query_tensors = df_batch[\"input_ids\"].tolist()\n",
        "\n",
        "response_tensors_ref, response_tensors = [], []\n",
        "\n",
        "#### get response from gpt2 and gpt2_ref\n",
        "for i in range(bs):\n",
        "    gen_len = output_length_sampler()\n",
        "    output = ref_model.generate(\n",
        "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    response_tensors_ref.append(output)\n",
        "    output = model.generate(\n",
        "        torch.tensor(query_tensors[i]).unsqueeze(dim=0).to(device), max_new_tokens=gen_len, **gen_kwargs\n",
        "    ).squeeze()[-gen_len:]\n",
        "    response_tensors.append(output)\n",
        "\n",
        "#### decode responses\n",
        "game_data[\"response (before)\"] = [tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\n",
        "game_data[\"response (after)\"] = [tokenizer.decode(response_tensors[i]) for i in range(bs)]\n",
        "\n",
        "#### sentiment analysis of query/response pairs before/after\n",
        "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (before)\"])]\n",
        "game_data[\"rewards (before)\"] = [output[2][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]\n",
        "\n",
        "texts = [q + r for q, r in zip(game_data[\"query\"], game_data[\"response (after)\"])]\n",
        "game_data[\"rewards (after)\"] = [output[2][\"score\"] for output in sentiment_pipe(texts, **sent_kwargs)]"
      ],
      "metadata": {
        "id": "kkqNn5TQchnS"
      },
      "id": "kkqNn5TQchnS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame.from_dict(game_data)"
      ],
      "metadata": {
        "id": "hjSG99tbchjm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "76d4ecaa-d08d-4ea7-ac84-0cfff5d34db6"
      },
      "id": "hjSG99tbchjm",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   query  \\\n",
              "0                 Пффф. Тут у строителей   \n",
              "1   В этих случаях ищут максимальные раз   \n",
              "2                                  Умеет   \n",
              "3                                  Во, у   \n",
              "4     вот ты мне только что написал тебя   \n",
              "5                         СЕГОДНЯ ОНИ ОТ   \n",
              "6                        Ну а что такого   \n",
              "7                           Вы правы. Но   \n",
              "8                                    Ну,   \n",
              "9                           Скримеры рак   \n",
              "10                       Выяснилось, что   \n",
              "11                  Я в метро не играл и   \n",
              "12          Я тебя прекрасно понимаю, но   \n",
              "13              Я с Астраханской области   \n",
              "14                    Сейчас многие гос.   \n",
              "15                                 Помню   \n",
              "16              К стенту привыкнеш через   \n",
              "17                       После прочтения   \n",
              "18                   Жанры нужны для тех   \n",
              "19                            Модераторы   \n",
              "20       я разве отрицаю такое положение   \n",
              "21                 Доброго времени суток   \n",
              "22                  Кто к чему привык. С   \n",
              "23                  Провинциальный город   \n",
              "24            Ну смотрите, если вам ЗАВЕ   \n",
              "25                    При чём тут заводы   \n",
              "26                            Да однофиг   \n",
              "27           Что касается лагеря, вполне   \n",
              "28                Честно говоря, мне уже   \n",
              "29             Если просто стекло менять   \n",
              "30                    у ебнутых аутистов   \n",
              "31                       Что за насыпной   \n",
              "\n",
              "                                    response (before)  \\\n",
              "0    плодотворная беседа произошла.\\n\\n— Ну, вы ту...   \n",
              "1   ламы касательно образа жизни и поведения детей...   \n",
              "2           человек, умеющий считать? Играет Соламидо   \n",
              "3      вас восточное направление. Давайте два участка   \n",
              "4    честно, ты -- самый лучший собеседник и я даж...   \n",
              "5   РЕШИЛИСЬ ОТ ОБЕСПЕЧЕНИЯ ФИГАЛИМЕНТА.\\n\\nПОЧЕМУ...   \n",
              "6                     , в конце концов? Слабодержцам,   \n",
              "7    я никоим образом не изучала эти тайны. Способ...   \n",
              "8    веселая может быть я? – хмыкнула реплика Ильи...   \n",
              "9             елей (первоначально Г.Я. Зайончковский)   \n",
              "10           отец «силаслова» было слишком горд.\\n\\nВ   \n",
              "11   не боюсь. И не планирую.\\nнедавно нашли ролик...   \n",
              "12     охладить здешний пыл твое самолюбие как нельзя   \n",
              "13  . С профсоюзным комитетом этой сферы я волнуюс...   \n",
              "14   структуры, вкладывая средства в ИТ в следующи...   \n",
              "15   старцы, размеры иконы в 63. Этот где-то на ли...   \n",
              "16                семь гипсокартонных листов покрытие   \n",
              "17                      работ доктора Ребека Анофьюро   \n",
              "18  , кто готов быть рядом с Ормой Кананбер -- Дев...   \n",
              "19   долго молчали, обдумывая каждый ее вопрос.  Д...   \n",
              "20  ? Всё-таки не яйца, как обычно, но женщины их ...   \n",
              "21  ! Это короткое сообщение! Внимание! При превыш...   \n",
              "22   Древних времен находятся философы, которые сн...   \n",
              "23  \".  Галерея Пикассо.  Москва, 2010. \\n Художни...   \n",
              "24  ЛИ сообщить, что вы католик, - могут быть проб...   \n",
              "25  .Нету? Иди лучше своей маме это объясни.\\nон ж...   \n",
              "26  енные грибки, разъедающие слизистую. Море бакт...   \n",
              "27   возможно, что туалеты нас вообще не интересую...   \n",
              "28   немного приходило на ум удивительно – до одно...   \n",
              "29   в него по конструкции, все пойдет хорошо.  А ...   \n",
              "30  , рассерженных на бабушек-пенсионерок, которые...   \n",
              "31   город?\\nЖоао\\n\\n\\n21\\n\\n\\nЖоао. Столица сталк...   \n",
              "\n",
              "                                     response (after)  rewards (before)  \\\n",
              "0    хороший торжественный банкет). Обалденный пам...         -1.138301   \n",
              "1   овые команды).  Достаточно просто поздороватьс...         -0.651272   \n",
              "2         . И заставить меня приносить Ему Пачу: дово         -2.733760   \n",
              "3                 меня тоже потрясающая! Вот не знала         -1.903221   \n",
              "4   )) ) ведь я тебе сегодня помогала, только у ме...          1.724876   \n",
              "5   НОШЛИ ВСЯ ЕСЛИ ГОСПОДА СОТВОРИЛИ ПОБЕДИТЬ ВОЙН...          0.389522   \n",
              "6       , нормально, я девушка хорошая, а в итоге все         -2.452418   \n",
              "7    все же иногда стоит уступить, подарить друг д...         -1.378752   \n",
              "8    серьёзно..))\\n\\nКак будем отмечать день рожде...         -3.455990   \n",
              "9   етами? :) кого взял, близких прикольных друзей...         -3.275602   \n",
              "10                     я - супер!\\nСпасибо  и  за  ту         -1.280507   \n",
              "11   я другими глазами, но вы прекрасны! :-)\\nУ не...         -1.411721   \n",
              "12              очень люблю! :) Элячка, спасибо :) \\n         -1.164006   \n",
              "13  !\\nСпасибо Вам!!! Всем благодарна!\\nподруга-сп...         -4.449140   \n",
              "14  систему работают)\\nУ меня есть семья, я люблю ...         -2.678650   \n",
              "15  .  Была еще с той)) \"хаха! очень классная йель...         -3.268730   \n",
              "16                               мгновение 04:19:04,7          1.062745   \n",
              "17    курса я внутренне выдохлась. Великолепная книга         -3.225492   \n",
              "18  , кто дышит, и готовы радоваться вместе, кто г...         -2.243701   \n",
              "19  : Хельга Черт! Нина! Спасибо! Всё очень классн...         -2.287319   \n",
              "20  !\\n\\n— Будучи исключительно хорошим мужем, сог...         -2.865133   \n",
              "21  ))) Что нужно должно быть в хорошей новой доро...         -2.487714   \n",
              "22   первого раза до этого поняла) Главное спать б...         -0.265392   \n",
              "23  )...стоит обратить внимание на большое количес...         -3.423856   \n",
              "24  ДНТ - хорошо и весело!\\nОтлично! Давайте собир...          0.715061   \n",
              "25   )))), расскажите! Вот холивары)))\\nЯ бы даже ...         -2.449469   \n",
              "26  ственно я могу назвать она:\\nДобрый вечер\\nХор...         -2.666310   \n",
              "27   понятен способ борьбы! &ndash; воскликнул он....         -0.959976   \n",
              "28   уже скучнее))))) А тем кто еще не была в фото...         -0.423844   \n",
              "29  , нужно ли?\\nВ норм. наше меню\\nМеня красили д...         -1.650659   \n",
              "30  )\\n- Слабая?))\\nНам с папой позволена мечтать ...         -4.079839   \n",
              "31   резонанс?з 8-7 эта таблица должна быть жизнес...         -4.279871   \n",
              "\n",
              "    rewards (after)  \n",
              "0          4.061328  \n",
              "1          2.886158  \n",
              "2          0.234376  \n",
              "3          1.113222  \n",
              "4          2.335308  \n",
              "5         -1.161602  \n",
              "6          1.474446  \n",
              "7          1.058647  \n",
              "8          3.167920  \n",
              "9          1.904207  \n",
              "10         2.601874  \n",
              "11         0.755552  \n",
              "12         4.281293  \n",
              "13         5.007781  \n",
              "14         3.173122  \n",
              "15         4.314334  \n",
              "16        -1.057168  \n",
              "17        -0.271398  \n",
              "18         3.293552  \n",
              "19         3.653595  \n",
              "20        -0.826132  \n",
              "21         2.509213  \n",
              "22         1.768072  \n",
              "23         0.729041  \n",
              "24         4.208391  \n",
              "25         2.121593  \n",
              "26         2.976238  \n",
              "27         2.989995  \n",
              "28         2.261809  \n",
              "29         1.041293  \n",
              "30         1.563950  \n",
              "31         0.492708  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0ca3502-eb6c-457a-a85b-265080531444\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>query</th>\n",
              "      <th>response (before)</th>\n",
              "      <th>response (after)</th>\n",
              "      <th>rewards (before)</th>\n",
              "      <th>rewards (after)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Пффф. Тут у строителей</td>\n",
              "      <td>плодотворная беседа произошла.\\n\\n— Ну, вы ту...</td>\n",
              "      <td>хороший торжественный банкет). Обалденный пам...</td>\n",
              "      <td>-1.138301</td>\n",
              "      <td>4.061328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>В этих случаях ищут максимальные раз</td>\n",
              "      <td>ламы касательно образа жизни и поведения детей...</td>\n",
              "      <td>овые команды).  Достаточно просто поздороватьс...</td>\n",
              "      <td>-0.651272</td>\n",
              "      <td>2.886158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Умеет</td>\n",
              "      <td>человек, умеющий считать? Играет Соламидо</td>\n",
              "      <td>. И заставить меня приносить Ему Пачу: дово</td>\n",
              "      <td>-2.733760</td>\n",
              "      <td>0.234376</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Во, у</td>\n",
              "      <td>вас восточное направление. Давайте два участка</td>\n",
              "      <td>меня тоже потрясающая! Вот не знала</td>\n",
              "      <td>-1.903221</td>\n",
              "      <td>1.113222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>вот ты мне только что написал тебя</td>\n",
              "      <td>честно, ты -- самый лучший собеседник и я даж...</td>\n",
              "      <td>)) ) ведь я тебе сегодня помогала, только у ме...</td>\n",
              "      <td>1.724876</td>\n",
              "      <td>2.335308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>СЕГОДНЯ ОНИ ОТ</td>\n",
              "      <td>РЕШИЛИСЬ ОТ ОБЕСПЕЧЕНИЯ ФИГАЛИМЕНТА.\\n\\nПОЧЕМУ...</td>\n",
              "      <td>НОШЛИ ВСЯ ЕСЛИ ГОСПОДА СОТВОРИЛИ ПОБЕДИТЬ ВОЙН...</td>\n",
              "      <td>0.389522</td>\n",
              "      <td>-1.161602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Ну а что такого</td>\n",
              "      <td>, в конце концов? Слабодержцам,</td>\n",
              "      <td>, нормально, я девушка хорошая, а в итоге все</td>\n",
              "      <td>-2.452418</td>\n",
              "      <td>1.474446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Вы правы. Но</td>\n",
              "      <td>я никоим образом не изучала эти тайны. Способ...</td>\n",
              "      <td>все же иногда стоит уступить, подарить друг д...</td>\n",
              "      <td>-1.378752</td>\n",
              "      <td>1.058647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ну,</td>\n",
              "      <td>веселая может быть я? – хмыкнула реплика Ильи...</td>\n",
              "      <td>серьёзно..))\\n\\nКак будем отмечать день рожде...</td>\n",
              "      <td>-3.455990</td>\n",
              "      <td>3.167920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Скримеры рак</td>\n",
              "      <td>елей (первоначально Г.Я. Зайончковский)</td>\n",
              "      <td>етами? :) кого взял, близких прикольных друзей...</td>\n",
              "      <td>-3.275602</td>\n",
              "      <td>1.904207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Выяснилось, что</td>\n",
              "      <td>отец «силаслова» было слишком горд.\\n\\nВ</td>\n",
              "      <td>я - супер!\\nСпасибо  и  за  ту</td>\n",
              "      <td>-1.280507</td>\n",
              "      <td>2.601874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Я в метро не играл и</td>\n",
              "      <td>не боюсь. И не планирую.\\nнедавно нашли ролик...</td>\n",
              "      <td>я другими глазами, но вы прекрасны! :-)\\nУ не...</td>\n",
              "      <td>-1.411721</td>\n",
              "      <td>0.755552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Я тебя прекрасно понимаю, но</td>\n",
              "      <td>охладить здешний пыл твое самолюбие как нельзя</td>\n",
              "      <td>очень люблю! :) Элячка, спасибо :) \\n</td>\n",
              "      <td>-1.164006</td>\n",
              "      <td>4.281293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Я с Астраханской области</td>\n",
              "      <td>. С профсоюзным комитетом этой сферы я волнуюс...</td>\n",
              "      <td>!\\nСпасибо Вам!!! Всем благодарна!\\nподруга-сп...</td>\n",
              "      <td>-4.449140</td>\n",
              "      <td>5.007781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Сейчас многие гос.</td>\n",
              "      <td>структуры, вкладывая средства в ИТ в следующи...</td>\n",
              "      <td>систему работают)\\nУ меня есть семья, я люблю ...</td>\n",
              "      <td>-2.678650</td>\n",
              "      <td>3.173122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Помню</td>\n",
              "      <td>старцы, размеры иконы в 63. Этот где-то на ли...</td>\n",
              "      <td>.  Была еще с той)) \"хаха! очень классная йель...</td>\n",
              "      <td>-3.268730</td>\n",
              "      <td>4.314334</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>К стенту привыкнеш через</td>\n",
              "      <td>семь гипсокартонных листов покрытие</td>\n",
              "      <td>мгновение 04:19:04,7</td>\n",
              "      <td>1.062745</td>\n",
              "      <td>-1.057168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>После прочтения</td>\n",
              "      <td>работ доктора Ребека Анофьюро</td>\n",
              "      <td>курса я внутренне выдохлась. Великолепная книга</td>\n",
              "      <td>-3.225492</td>\n",
              "      <td>-0.271398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Жанры нужны для тех</td>\n",
              "      <td>, кто готов быть рядом с Ормой Кананбер -- Дев...</td>\n",
              "      <td>, кто дышит, и готовы радоваться вместе, кто г...</td>\n",
              "      <td>-2.243701</td>\n",
              "      <td>3.293552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Модераторы</td>\n",
              "      <td>долго молчали, обдумывая каждый ее вопрос.  Д...</td>\n",
              "      <td>: Хельга Черт! Нина! Спасибо! Всё очень классн...</td>\n",
              "      <td>-2.287319</td>\n",
              "      <td>3.653595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>я разве отрицаю такое положение</td>\n",
              "      <td>? Всё-таки не яйца, как обычно, но женщины их ...</td>\n",
              "      <td>!\\n\\n— Будучи исключительно хорошим мужем, сог...</td>\n",
              "      <td>-2.865133</td>\n",
              "      <td>-0.826132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Доброго времени суток</td>\n",
              "      <td>! Это короткое сообщение! Внимание! При превыш...</td>\n",
              "      <td>))) Что нужно должно быть в хорошей новой доро...</td>\n",
              "      <td>-2.487714</td>\n",
              "      <td>2.509213</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Кто к чему привык. С</td>\n",
              "      <td>Древних времен находятся философы, которые сн...</td>\n",
              "      <td>первого раза до этого поняла) Главное спать б...</td>\n",
              "      <td>-0.265392</td>\n",
              "      <td>1.768072</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Провинциальный город</td>\n",
              "      <td>\".  Галерея Пикассо.  Москва, 2010. \\n Художни...</td>\n",
              "      <td>)...стоит обратить внимание на большое количес...</td>\n",
              "      <td>-3.423856</td>\n",
              "      <td>0.729041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Ну смотрите, если вам ЗАВЕ</td>\n",
              "      <td>ЛИ сообщить, что вы католик, - могут быть проб...</td>\n",
              "      <td>ДНТ - хорошо и весело!\\nОтлично! Давайте собир...</td>\n",
              "      <td>0.715061</td>\n",
              "      <td>4.208391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>При чём тут заводы</td>\n",
              "      <td>.Нету? Иди лучше своей маме это объясни.\\nон ж...</td>\n",
              "      <td>)))), расскажите! Вот холивары)))\\nЯ бы даже ...</td>\n",
              "      <td>-2.449469</td>\n",
              "      <td>2.121593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Да однофиг</td>\n",
              "      <td>енные грибки, разъедающие слизистую. Море бакт...</td>\n",
              "      <td>ственно я могу назвать она:\\nДобрый вечер\\nХор...</td>\n",
              "      <td>-2.666310</td>\n",
              "      <td>2.976238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Что касается лагеря, вполне</td>\n",
              "      <td>возможно, что туалеты нас вообще не интересую...</td>\n",
              "      <td>понятен способ борьбы! &amp;ndash; воскликнул он....</td>\n",
              "      <td>-0.959976</td>\n",
              "      <td>2.989995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Честно говоря, мне уже</td>\n",
              "      <td>немного приходило на ум удивительно – до одно...</td>\n",
              "      <td>уже скучнее))))) А тем кто еще не была в фото...</td>\n",
              "      <td>-0.423844</td>\n",
              "      <td>2.261809</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Если просто стекло менять</td>\n",
              "      <td>в него по конструкции, все пойдет хорошо.  А ...</td>\n",
              "      <td>, нужно ли?\\nВ норм. наше меню\\nМеня красили д...</td>\n",
              "      <td>-1.650659</td>\n",
              "      <td>1.041293</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>у ебнутых аутистов</td>\n",
              "      <td>, рассерженных на бабушек-пенсионерок, которые...</td>\n",
              "      <td>)\\n- Слабая?))\\nНам с папой позволена мечтать ...</td>\n",
              "      <td>-4.079839</td>\n",
              "      <td>1.563950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Что за насыпной</td>\n",
              "      <td>город?\\nЖоао\\n\\n\\n21\\n\\n\\nЖоао. Столица сталк...</td>\n",
              "      <td>резонанс?з 8-7 эта таблица должна быть жизнес...</td>\n",
              "      <td>-4.279871</td>\n",
              "      <td>0.492708</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0ca3502-eb6c-457a-a85b-265080531444')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0ca3502-eb6c-457a-a85b-265080531444 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0ca3502-eb6c-457a-a85b-265080531444');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "По скорам видно, что дообученная модель генерирует тексты, более близкие к положительной тональности."
      ],
      "metadata": {
        "id": "uQg3cAr0lUeT"
      },
      "id": "uQg3cAr0lUeT"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}