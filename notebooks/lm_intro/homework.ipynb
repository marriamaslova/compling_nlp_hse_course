{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marriamaslova/compling_nlp_hse_course/blob/master/notebooks/lm_intro/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00fad453",
      "metadata": {
        "id": "00fad453"
      },
      "source": [
        "# Домашнее задание № 4. Языковые модели"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d056af4",
      "metadata": {
        "id": "5d056af4"
      },
      "source": [
        "## Задание 1 (8 баллов)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1f532a8",
      "metadata": {
        "id": "d1f532a8"
      },
      "source": [
        "В семинаре для генерации мы использовали предположение маркова и считали, что слово зависит только от 1 предыдущего слова. Но ничто нам не мешает попробовать увеличить размер окна и учитывать два или даже три прошлых слова. Для них мы еще сможем собрать достаточно статистик и, логично предположить, что качество сгенерированного текста должно вырасти."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de743d1d",
      "metadata": {
        "id": "de743d1d"
      },
      "source": [
        "Попробуйте сделать языковую модель, которая будет учитывать два предыдущих слова при генерации текста.\n",
        "Сгенерируйте несколько текстов (3-5) и расчитайте перплексию получившейся модели. \n",
        "Можно использовать данные из семинара или любые другие (сопоставимые или большие по объему). Перплексию рассчитывайте на 10-50 отложенных предложениях (они не должны использоваться при сборе статистик).\n",
        "\n",
        "\n",
        "Подсказки:  \n",
        "    - нужно будет добавить еще один тэг <start>  \n",
        "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы  \n",
        "    - тексты должны быть очень похожи на нормальные (если у вас получается рандомная каша, вы что-то делаете не так). "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install razde"
      ],
      "metadata": {
        "id": "adxxS5Dj_yMi"
      },
      "id": "adxxS5Dj_yMi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import numpy as np\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML\n",
        "from collections import Counter\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from scipy.sparse import lil_matrix"
      ],
      "metadata": {
        "id": "lea8IDKU_yJz"
      },
      "id": "lea8IDKU_yJz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dvach = open('2ch_corpus.txt').read()\n",
        "news = open('lenta.txt').read()"
      ],
      "metadata": {
        "id": "sfeV82EH_yHM"
      },
      "id": "sfeV82EH_yHM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    normalized_text = [word.text.strip(punctuation) for word \\\n",
        "                                                            in razdel_tokenize(text)]\n",
        "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
        "    return normalized_text"
      ],
      "metadata": {
        "id": "zjCCqBji_yEu"
      },
      "id": "zjCCqBji_yEu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "norm_dvach = normalize(dvach)\n",
        "norm_news = normalize(news)"
      ],
      "metadata": {
        "id": "sfrpRbQM_yCW"
      },
      "id": "sfrpRbQM_yCW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_dvach = Counter(norm_dvach)\n",
        "vocab_news = Counter(norm_news)"
      ],
      "metadata": {
        "id": "d-tTcIVL_x_1"
      },
      "id": "d-tTcIVL_x_1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
        "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})"
      ],
      "metadata": {
        "id": "kZedTw8S_x9I"
      },
      "id": "kZedTw8S_x9I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "metadata": {
        "id": "hdhzrvsW_x6i"
      },
      "id": "hdhzrvsW_x6i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_dvach = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]\n",
        "sentences_news = [['<start>'] + ['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
      ],
      "metadata": {
        "id": "rwUPFrtP_x38"
      },
      "id": "rwUPFrtP_x38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams_dvach = Counter()\n",
        "bigrams_dvach = Counter()\n",
        "trigrams_dvach = Counter()\n",
        "\n",
        "for sentence in sentences_dvach:\n",
        "    unigrams_dvach.update(sentence)\n",
        "    bigrams_dvach.update(ngrammer(sentence))\n",
        "    trigrams_dvach.update(ngrammer(sentence, 3))\n",
        "\n",
        "unigrams_news = Counter()\n",
        "bigrams_news = Counter()\n",
        "trigrams_news = Counter()\n",
        "\n",
        "for sentence in sentences_news:\n",
        "    unigrams_news.update(sentence)\n",
        "    bigrams_news.update(ngrammer(sentence))\n",
        "    trigrams_news.update(ngrammer(sentence, 3))"
      ],
      "metadata": {
        "id": "t8Uj1U17_x1Y"
      },
      "id": "t8Uj1U17_x1Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "4jWl4q69_xyu"
      },
      "id": "4jWl4q69_xyu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_dvach = lil_matrix((len(bigrams_dvach), \n",
        "                   len(unigrams_dvach)))\n",
        "\n",
        "id2word_dvach = list(unigrams_dvach)\n",
        "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
        "id2bigram_dvach = list(bigrams_dvach)\n",
        "bigram2id_dvach = {bigram:i for i, bigram in enumerate(id2bigram_dvach)}\n",
        "\n",
        "for ngram in trigrams_dvach:\n",
        "    word1, word2, word3 = ngram.split()\n",
        "    trigram = word1, word2, word3\n",
        "    bigram = word1, word2\n",
        "    bigram_d = ' '.join(bigram)\n",
        "    trigram_d = ' '.join(trigram)\n",
        "    matrix_dvach[bigram2id_dvach[bigram_d], word2id_dvach[word3]] = (trigrams_dvach[trigram_d]/bigrams_dvach[bigram_d]) "
      ],
      "metadata": {
        "id": "TrgvZLgX_xwL"
      },
      "id": "TrgvZLgX_xwL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_news = lil_matrix((len(bigrams_news), \n",
        "                   len(unigrams_news)))\n",
        "\n",
        "id2word_news = list(unigrams_news)\n",
        "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
        "id2bigram_news = list(bigrams_news)\n",
        "bigram2id_news = {bigram:i for i, bigram in enumerate(id2bigram_news)}\n",
        "\n",
        "for ngram in trigrams_news:\n",
        "    word1, word2, word3 = ngram.split()\n",
        "    trigram = word1, word2, word3\n",
        "    bigram = word1, word2\n",
        "    bigram_n = ' '.join(bigram)\n",
        "    trigram_n = ' '.join(trigram)\n",
        "    matrix_news[bigram2id_news[bigram_n], word2id_news[word3]] =  (trigrams_news[trigram_n]/\n",
        "                                                                     bigrams_news[bigram_n])"
      ],
      "metadata": {
        "id": "OiKhpqXD_xts"
      },
      "id": "OiKhpqXD_xts",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(matrix, id2word, word2id, bigram2id, n=100, start='<start>, <start>'):\n",
        "    text = []\n",
        "    current_idx = bigram2id[start]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
        "        text.append(id2word[chosen])\n",
        "        \n",
        "        if id2word[chosen] == '<end>':\n",
        "            chosen = bigram2id['<start>, <start>']\n",
        "        current_idx = chosen\n",
        "    \n",
        "    return ' '.join(text)"
      ],
      "metadata": {
        "id": "mjqnKJjJ_xrT"
      },
      "id": "mjqnKJjJ_xrT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perplexity(logp, N):\n",
        "    return np.exp((-1/N) * logp)"
      ],
      "metadata": {
        "id": "6bLRoSUT_xos"
      },
      "id": "6bLRoSUT_xos",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_joint_proba(text, word_probas):\n",
        "    prob = 0\n",
        "    tokens = normalize(text)\n",
        "    for word in tokens:\n",
        "        if word in word_probas:\n",
        "            prob += (np.log(word_probas[word]))\n",
        "        else:\n",
        "            prob += np.log(2e-4)\n",
        "    \n",
        "    return prob, len(tokens)"
      ],
      "metadata": {
        "id": "0YropZ0o_xmI"
      },
      "id": "0YropZ0o_xmI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_1 = 'Ранее появилась информация, что Twitter заблокировал аккаунты некоторых журналистов, в их числе корреспонденты телеканала CNN и газеты The Washington Post. Эти санкции были связаны с решением Twitter о запрете передавать текущее местоположение другого человека без его согласия. Лишившиеся доступа к аккаунтам журналисты писали о новой политике Маска, которая, по его словам, была вызвана инцидентом с преследованием его семьи во вторник вечером в Лос-Анджелесе. Бизнесмен позже провел опрос среди пользователей социальной сети по поводу разблокировки журналистов. Большая часть опрошенных (43%) высказались за немедленное разблокирование аккаунтов журналистов. Однако уже после публикации данных опроса Маск дезавуировал его результаты и заявил, что проведет опроc заново.'"
      ],
      "metadata": {
        "id": "pjjwS3Oh_xjn"
      },
      "id": "pjjwS3Oh_xjn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(*compute_joint_proba(phrase_1, probas_dvach))\n",
        "#13539.339030311741"
      ],
      "metadata": {
        "id": "msvX1weH_xg-"
      },
      "id": "msvX1weH_xg-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(*compute_joint_proba(phrase_1, probas_news))\n",
        "#5553.426834760291"
      ],
      "metadata": {
        "id": "ai0gWM97_xep"
      },
      "id": "ai0gWM97_xep",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrase_2 = 'Долбимся в ФУТочко. Притворяемся, что сделали 11 побед и подарили 9 оставшихся игр, хвалимся наградами, пулами, тимами, обсуждаем тактики. Но самое главное - кидаемся какашками друг в друга, по тому что кто-то играет не так, как нам хочется. Только пару дней им играю, обычно он на подыгрыше у меня оказывается , тот же смолярек лучше голы клепал , это наверное мои стиль игры такой , я второй волной забиваю, редко когда убегаю.Здесь мы жёстко бомбим на решение руководства клуба уволить Томаса Тухеля из-за нежелания покупать сорокалетнюю Кристину и поминаем его короткую, но великую эру в клубе - эру, когда Челси побывал в финалах почти всех соревнований, в которых он участвовал, и эру, в которой число 3 стало настоящим талисманом для клуба.'"
      ],
      "metadata": {
        "id": "Oh8hiDnEAUD-"
      },
      "id": "Oh8hiDnEAUD-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perplexity(*compute_joint_proba(phrase_2, probas_dvach))\n",
        "#5750.225393577827"
      ],
      "metadata": {
        "id": "0KCEiitEAT9s"
      },
      "id": "0KCEiitEAT9s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6afcef88",
      "metadata": {
        "id": "6afcef88"
      },
      "outputs": [],
      "source": [
        "perplexity(*compute_joint_proba(phrase_2, probas_news))\n",
        "#5369.64628948354"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e0a8dd5",
      "metadata": {
        "id": "8e0a8dd5"
      },
      "source": [
        "## Задание № 2* (2 балла). "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b36c44b",
      "metadata": {
        "id": "0b36c44b"
      },
      "source": [
        "Прочитайте главу про языковое моделирование в книге Журафски и Мартина - https://web.stanford.edu/~jurafsky/slp3/3.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9b1bd8",
      "metadata": {
        "id": "5d9b1bd8"
      },
      "source": [
        "Развернуто (в пределах 1000 знаков) ответьте на вопросы (по-русски):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c2cf844",
      "metadata": {
        "id": "0c2cf844"
      },
      "source": [
        "1. Что можно делать с проблемой несловарных слов? В семинаре мы просто использовали какое-то маленькое значение вероятности, а какие есть другие способы?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d1c152",
      "metadata": {
        "id": "d1d1c152"
      },
      "source": [
        "2. Что такое сглаживание (smoothing)?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}