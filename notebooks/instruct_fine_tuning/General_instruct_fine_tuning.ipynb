{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8201db42",
   "metadata": {},
   "source": [
    "# General fine-tuning of text generation models through instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3925e6",
   "metadata": {},
   "source": [
    "В предыдущих семинарах мы в основном фокусировались на какой-то одной задаче - исправление опечаток, выделение сущностей, определение тональности, машинный перевод и т.п. Мы двигались от самых простых моделей к предобученным трансформерам, но подход в целом не менялся - мы брали модель и тренировали её решать нужную задачу, а когда нужно было решить новую задачу мы откладывали старую модель и тренировали новую. \n",
    "\n",
    "Один из последних быстро развивающихся трендов в NLP - решать множество задач 1 общей моделью. Если задуматься, то все к этому шло:\n",
    "\n",
    "1) Self-supervised предобучение научились более менее стабильно масштабировать на огромный текстовые корпуса, что дало нам модели, в которых уже заложено очень широкое понимание языка\n",
    "2) Fine-tuning предобученных моделей под конкретную задачу требует небольшое количество примеров (сотни или даже десятки\n",
    "3) При правильных промтах предобученный модели могли решать даже задачи, которые они никогда не видели (zero-shot и in context learning)\n",
    "4) Все возможные NLP задачи стали сводится к генерации текста (классификация - генерация одного токена, исправление опечаток - генерация исправленной последовательности, выделение сущности - генерация именованых сущностей нужного типа, даже генерация кода теперь решается просто генерацией)\n",
    "\n",
    "\n",
    "Поэтому было вопросом времени, когда кто-то попробует обучить модель решать сразу все нужные задачи и у них получится.\n",
    "\n",
    "Текущее топовое решение - дообучить модель на датесете разнообразных инструкций, которые соответствуют нужным задачам, а затем дотренировать новую модель с помощью Reinforcement Learning from Human Feedback (RLHF). Такой подход первым успешно применил OpenAI и результат можно наблюдать в ChatGPT (спойлер: результат очень хороший). Но OpenAI не опубликовал в открытом доступе ни модели, ни код ни какие-то технические описания их подхода. Поэтому сейчас болшАя часть исследовательского сообщества в NLP занимается тем, что пытается воспроизвести chatgpt по общем описаниям, которые раскрыл OpenAI. И очень многое уже получилось воспроизвести и буквально с каждым днем такие модели становятся меньше/дешевле и доступнее.\n",
    "\n",
    "В этом семинаре мы попробуем дообучить модель на датасете инструкций. Но прежде чем переходить к этому, давайте посмотрим на две статьи (и модели), которые есть в открытом доступе и которые сильно повлияли на движение в сторону общих моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6e77773",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %pip install pandas transformers tokenizers datasets xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110460fd",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6796cb",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd9deeb",
   "metadata": {},
   "source": [
    "Первая статья Т5 (Text-To-Text Transfer Transformer, https://arxiv.org/abs/1910.10683, Google Research, конец 2019 года) \n",
    "Это очень большая статья, в которой подробно исследовалась унификация различных NLP задач в задачу генерации. А также они попробовали много различных подходов к предобучению (в то время выходило очень много статей, которые как-то меняли self-supervised задачу и они попробовали много разных комбинаций, чтобы получить хорошую модель). В результате у них получилось несколько вариантов модели Т5 и всех их они выложили в открытый доступ. \n",
    "Еще в статье они пробовали тюнить модель под разные задачи, но по большей части все еще по отдельности. Если вы прочитаете статью или хотя бы описание fine-tuning экспериментов, то увидите, что на тот момент парадигма (1 модель - 1 задача) еще не изменилась. В своих экспериментах они пробовали тренироваться сразу под несколько задач, но у них было не достаточно много разнообразных задач и в итоге общая модель работала хуже на отдельных задачах, чем специфичные модели.\n",
    "\n",
    "Но в открытый доступ они выложили в том числе и модели, которые были дообучены на нескольких задачах. Задача в модель передается через префикс (посмотрите на начало примеров выше). Эти модели есть на huggingface, давайте попробуем взять какую-то модель и попробовать сходу решить задачу саммаризации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "084c3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c4ca0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6c6f5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = 't5-large'\n",
    "MODEL_NAME = 't5-base'\n",
    "# MODEL_NAME = 't5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "196314f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c8d7f",
   "metadata": {},
   "source": [
    "Возьмем какой-нибудь текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8b7a671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_prefix = \"summarize: {}\"\n",
    "\n",
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1206e75",
   "metadata": {},
   "source": [
    "С моделями в huggingface удобнее всего работать через torch, но это не страшно, так как все основные вещи реализованы в transformers и они одинаковые для torch и tf. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802f8ed",
   "metadata": {},
   "source": [
    "Попробуем сгенерировать саммари"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8902a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer([task_prefix.format(text)], \n",
    "                    return_tensors=\"pt\", padding=True)\n",
    "\n",
    "output_sequences = model.generate(\n",
    "    # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "    num_beams=5,\n",
    "    max_length=100,\n",
    "    no_repeat_ngram_size=3, \n",
    "#     repetition_penalty= 5.0,\n",
    "#     length_penalty=0.01,\n",
    "#     early_stopping=True,\n",
    "#     do_sample=True, \n",
    "#     top_k=30, \n",
    "#     top_p=0.8, \n",
    "    early_stopping=True,\n",
    "#     num_return_sequences=3,\n",
    "    num_return_sequences= 1,\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c9efe30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a754287d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all trains halted on a busy line between den Bosch and boxtel. badgers dug into a dike carrying rails. the national railway company says the line will be out of service for at least a week.']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579e7d80",
   "metadata": {},
   "source": [
    "Работает неплохо, но конечно для реального практическо применения нужно тюнить модель дополнительно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfaf9e4",
   "metadata": {},
   "source": [
    "## FLAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628438c6",
   "metadata": {},
   "source": [
    "![](https://1.bp.blogspot.com/-_kPdaMrcRWI/YV2b-XFoRxI/AAAAAAAAIMw/KDjg0IfuoK8hjpSXNODoV46D8Rb5rK8hgCLcBGAsYHQ/w640-h178/image3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dac926d",
   "metadata": {},
   "source": [
    "Второя статья - FLAN (тоже от Google Research, тоже огромная, Finetuned Language Models Are Zero-Shot Learners, https://arxiv.org/abs/2109.01652, середина 2021 года)\n",
    "\n",
    "В этой статье уже заметен сдвиг в сторону общих моделей и уже сформировался подход к такому обучению через инструкции. Основная идея в статье - переделать различные NLP датасеты в большой датасет разнообразных инструкций (они сделали различные темплейты на правилах и прогнали их через размеченные датасеты) и обучить модель решать сразу всё. Инструкции при этом это не какие-то технические теги как в T5, а нормальные человеческие инструкии (буквально что-то вроде \"Translate this text from English to Russian\", \"Write five topics that describe this text\", \"What is the sentiment of this text? Options: Negative, Positive, Neutral.\"). При таком подходе они заметили, что модель начинает обобщаться на инструкции, которых она никогда не видела - так как модель предобучена на большом количестве текстов, она уже хорошо понимает язык и экстраполирует инструкции из обучающей выборки, используя свое понимание языка). И чем больше таких инструкций, тем лучше получалось.\n",
    "\n",
    "Они попробовали такой подход с разными моделями (T5, PALM) и везде получалось хорошо решать новые задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d37453a",
   "metadata": {},
   "source": [
    "FLAN варианты моделей также доступны на huggingface. Давайте попробуем с таким же текстом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "73456752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0d0f37f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1e1cb834",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'google/flan-t5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5a1cf26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd25ede8619841c9a4b41f95587aad76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "235243f25cbc49f6871dfbe6e12da901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a3f05d600f418c8b1e2bb9ba225498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf99eb8bee14c4bab2edccb19ac9364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e06ec94a07c4eb39b8da4a024561298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18328c7cf19f4551b346eaf85e40dcfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1bb66d205d49e3a28b0a826f70e790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115209a6",
   "metadata": {},
   "source": [
    "Инструкции модели можно передавать в свободном формате, поэтому сделаем функцию, чтобы удобнее было пробовать разные инструкции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ddc9bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruction, text, model):\n",
    "    \n",
    "\n",
    "    inputs = tokenizer([instruction.format(text)], \n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "        num_beams=5,\n",
    "        max_length=100,\n",
    "        no_repeat_ngram_size=3, \n",
    "    #     repetition_penalty= 5.0,\n",
    "    #     length_penalty=0.01,\n",
    "    #     early_stopping=True,\n",
    "    #     do_sample=True, \n",
    "    #     top_k=30, \n",
    "    #     top_p=0.8, \n",
    "        early_stopping=True,\n",
    "    #     num_return_sequences=3,\n",
    "        num_return_sequences= 1,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,  # disable sampling to test if batching affects output\n",
    "    )\n",
    "    summaries = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n",
    "    return summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f797fd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "41914df7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowing under rail tracks in the Netherlands have halted trains for at least a week.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a summary of this text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c9f6ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowed under rail tracks in northern and southern Netherlands, forcing lengthy cancellations on at least two lines'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a very short summary of this text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c04bcabb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowed under rail tracks in northern and southern Netherlands'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Write a headline for the following text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9cc58cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowing under rail tracks in the Netherlands'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Suggest a topic for this text. Text: {}\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79cc573",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82835cd2",
   "metadata": {},
   "source": [
    "## InstructGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98475c81",
   "metadata": {},
   "source": [
    "FLAN модели работали хорошо, но все еще недостаточно. OpenAI довел их до состояния, когда их можно использовать на практике. Они публиковали несколько статей и описаний своих экспериментов:\n",
    "\n",
    "https://openai.com/research/improving-language-model-behavior\n",
    "https://openai.com/research/instruction-following\n",
    "https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf\n",
    "https://openai.com/research/learning-to-summarize-with-human-feedback\n",
    "\n",
    "Они добавили еще одну важную часть - RLHF. Про нее мы попытаемся поговорить на следующем занятии. Пока сфокусируемся на инструкциях. Из описания OpenAI видно, что их подход очень похож на FLAN, но они машстабировали его и использовали для своих датасетов инструкции на основе реальных запросов к их API. И они продолжают это делать, исправляя все больше ошибок и нежелательных ответов. \n",
    "Также они сильно ускорились, когда добавили интерфейс (ChatGPT). Они даже говорили, что уже очень хорошая модель была доступна в их API около полугода и никто особо не обращал внимания на нее, хотя она уже работала как ChatGPT, но ей нужно было подавать правильный промпт. Когда они решили это через интерфейс (и промпт на бекенде), количество пользователей сильно увиличилось и к нем потекло очень много реальных запросов, на которых они быстро стали дообучаться."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75678946",
   "metadata": {},
   "source": [
    "Открытых моделей тут нет, поэтому перейдем к следующему шагу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57df25c3",
   "metadata": {},
   "source": [
    "## Alpaca "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38542bc2",
   "metadata": {},
   "source": [
    "Подробнее посмотрим на работу, которая вышла буквально на прошлой неделе - Stanford Alpaca \n",
    "![](https://crfm.stanford.edu/static/img/posts/2023-03-13-alpaca/alpaca_main.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f850c45",
   "metadata": {},
   "source": [
    "Код и датасет можно найти тут - https://github.com/tatsu-lab/stanford_alpaca\n",
    "Дальше код взят из train.py и немного изменен"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4619b6d4",
   "metadata": {},
   "source": [
    "Авторы Альпаки дообучили модель LLaMA (7 миллиардов параметров) на датасете инструкций, который они сгенерировали с помощью OpenAI API и получилась модель, которая очень похожа по качеству на саму модель от OpenAI.   \n",
    "\n",
    "LLaMa - это серия предобученных моделей от Meta. Они были опубликованы около месяца назад и Meta утверждает, что по метрикам их меньшие медоли сравнимы с GPT-3 (которая около 175 млрд параметров). Но Meta недавно сталкивалась с критикой за свою модель Galactica, которая была обучена на научных статьях. Сначала они выложили её в открытый доступ, но быстро оказалась, что она может генерировать псевдонаучные и лженаучные тексты и Meta быстро закрыла доступ к этой модели. Поэтому модель LLaMA не выложена в открытый доступ и имеет не комерческую лицензию. Чтобы получить модель, нужно заполнять специальную форму и ждать пока ее одобрят. Но естественно люди, которые получили доступ к модели начали выкладывать ее в открытый доступ - например, до сих пор висит ПР в либу Meta, в котором предлагается добавить в Readme.md ссылку на [торент](https://github.com/facebookresearch/llama/pull/73/commits/016a53608c5eae1021e171b9c4f06a9783fc14c0) \n",
    "\n",
    "Датасет инструкций они сгенерировали на основе статьи - https://arxiv.org/abs/2212.10560 И как они говорят у них ушло около 500$ на все, чтобы в тысячи раз дешевле того, что предполагается потратил сам OpenAI на свои модели. Но OpenAI запрещают использовать свои модели в таких целях и поэтому итоговую модель Alpaca они пока не выкладывают.\n",
    "\n",
    "Но они выложили в открытый доступ датасет и можно самому попробовать дообучить какую-то открытую предобученную модель."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980a1211",
   "metadata": {},
   "source": [
    "Скачаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01e0848c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-21 21:03:20--  https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 22773992 (22M) [text/plain]\n",
      "Saving to: ‘alpaca_data.json’\n",
      "\n",
      "alpaca_data.json    100%[===================>]  21.72M  38.9MB/s    in 0.6s    \n",
      "\n",
      "2023-03-21 21:03:21 (38.9 MB/s) - ‘alpaca_data.json’ saved [22773992/22773992]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3477de2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional, Dict, Sequence\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "# import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aeeaca",
   "metadata": {},
   "source": [
    "Посмотрим на датасет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279ba6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_alpaca = json.load(open('alpaca_data.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "563f1d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'instruction': 'Give three tips for staying healthy.',\n",
       "  'input': '',\n",
       "  'output': '1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \\n2. Exercise regularly to keep your body active and strong. \\n3. Get enough sleep and maintain a consistent sleep schedule.'},\n",
       " {'instruction': 'What are the three primary colors?',\n",
       "  'input': '',\n",
       "  'output': 'The three primary colors are red, blue, and yellow.'},\n",
       " {'instruction': 'Describe the structure of an atom.',\n",
       "  'input': '',\n",
       "  'output': 'An atom is made up of a nucleus, which contains protons and neutrons, surrounded by electrons that travel in orbits around the nucleus. The protons and neutrons have a positive charge, while the electrons have a negative charge, resulting in an overall neutral atom. The number of each particle determines the atomic number and the type of atom.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_alpaca[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec2955e",
   "metadata": {},
   "source": [
    "В нем каждый пример это инструкция, опциональный контекст и ответ.\n",
    "Для модели эти примеры еще оборачиваются в специальный промпт, который говорит модели, что она должна следовать инструкциям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1189c2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\"\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\"\n",
    "    ),\n",
    "    \"prompt_no_input\": (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
    "    ),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53688c8",
   "metadata": {},
   "source": [
    "Давайте попробуем дообучить модель от facebook - opt (она открытыя и устроена как LLama и GPT - это декодер онли модель)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3ff2ed",
   "metadata": {},
   "source": [
    "Далее код взят из гитхаба Alpaca и он на торче, но если поизучать его, то будет видно, что тут происходят те же манипуляции, что мы делали раньше (превращение токенов в индексы и паддинг/урезание последовательностей)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e8004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\"\"\"\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        )\n",
    "        for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "189e9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    targets: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"Preprocess the data by tokenizing.\"\"\"\n",
    "    examples = [s + t for s, t in zip(sources, targets)]\n",
    "    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]\n",
    "    input_ids = examples_tokenized[\"input_ids\"]\n",
    "    labels = copy.deepcopy(input_ids)\n",
    "    for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "        label[:source_len] = IGNORE_INDEX\n",
    "    return dict(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0672d",
   "metadata": {},
   "source": [
    "Далее это оборачивается к классы, которые предобрабатывают данные к формату huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93bf46bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "        list_data_dict = json.load(open(data_path))\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        prompt_input, prompt_no_input = PROMPT_DICT[\"prompt_input\"], PROMPT_DICT[\"prompt_no_input\"]\n",
    "        sources = [\n",
    "            prompt_input.format_map(example) if example.get(\"input\", \"\") != \"\" else prompt_no_input.format_map(example)\n",
    "            for example in list_data_dict\n",
    "        ]\n",
    "        targets = [f\"{example['output']}{tokenizer.eos_token}\" for example in list_data_dict]\n",
    "\n",
    "        logging.warning(\"Tokenizing inputs... This may take some time...\")\n",
    "        data_dict = preprocess(sources, targets, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e2217",
   "metadata": {},
   "source": [
    "Загружаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "851c73b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'facebook/opt-350m'\n",
    "model_name = \"facebook/opt-125m\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        max_length=512,\n",
    "        cache_dir=\"huggingface_cache\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad031e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir=\"huggingface_cache\",\n",
    "    model_max_length=512,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c5e09",
   "metadata": {},
   "source": [
    "Токенизируем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a617d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n",
      "WARNING:root:Tokenizing inputs... This may take some time...\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SupervisedDataset(tokenizer=tokenizer, data_path=\"alpaca_data.json\")\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f6954",
   "metadata": {},
   "source": [
    "Задаем параметры обуечения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b738630",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(learning_rate=1e-5, \n",
    "                 num_train_epochs=1,\n",
    "                 per_device_train_batch_size=2,\n",
    "                 gradient_accumulation_steps=1,\n",
    "                 evaluation_strategy='no',\n",
    "                 weight_decay=0.,\n",
    "                 warmup_ratio=0.03,\n",
    "                 lr_scheduler_type=\"cosine\",\n",
    "                 save_strategy='no',\n",
    "                 logging_steps=1000,\n",
    "                 output_dir=\"opt125_instruct_ft\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21d9783",
   "metadata": {},
   "source": [
    "И обучаем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f72ae726",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, \n",
    "                 tokenizer=tokenizer, \n",
    "                 args=train_args,\n",
    "                 train_dataset=train_dataset, \n",
    "                 eval_dataset=None, \n",
    "                 data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a77f94d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26001' max='26001' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26001/26001 39:30, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.332500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.230100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.162800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.120400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.141000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.136100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.130600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.094200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.068500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>2.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.108300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.032400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.042400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.989600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.019800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.974300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.005900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>1.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>1.977900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>1.970600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=26001, training_loss=2.066676487524158, metrics={'train_runtime': 2371.6488, 'train_samples_per_second': 21.927, 'train_steps_per_second': 10.963, 'total_flos': 3745819289088000.0, 'train_loss': 2.066676487524158, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1884439d",
   "metadata": {},
   "source": [
    "Сохраним модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "142a7410",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('opt125_ft_02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f652d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42bda0c1",
   "metadata": {},
   "source": [
    "И давайте попробуем ее на том же тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "44557cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d9696d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'opt125_ft_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "97779964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=512, max_length=512)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "707d718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_instruction(instruction, text, model):\n",
    "    text = text.replace('\\n', ' ')\n",
    "    prompt = (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "              \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "              f\"### Instruction:\\n{instruction}\\n\\n### Input:\\n{text}\\n\\n### Response:\")\n",
    "\n",
    "    inputs = tokenizer([prompt], \n",
    "                        return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        # this parameters are also important but you can read about them in the docs and just try changing them\n",
    "        num_beams=1,\n",
    "#         temperature=0.4,\n",
    "#         max_length=100,\n",
    "        max_new_tokens=20,\n",
    "#         no_repeat_ngram_size=3,\n",
    "    #     repetition_penalty= 5.0,\n",
    "    #     length_penalty=0.01,\n",
    "    #     early_stopping=True,\n",
    "    #     do_sample=True, \n",
    "    #     top_k=30, \n",
    "    #     top_p=0.8, \n",
    "        early_stopping=True,\n",
    "    #     num_return_sequences=3,\n",
    "        num_return_sequences= 1,\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,  # disable sampling to test if batching affects output\n",
    "    )\n",
    "    summaries = tokenizer.batch_decode(output_sequences[:,len(inputs[0]):], skip_special_tokens=True)\n",
    "    return summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "52ddc97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Badgers burrowing under rail tracks have halted trains in the northern and southern Netherlands, forcing lengthy cancellations on at least two lines.\n",
    "All trains were halted Tuesday afternoon on a busy line between the southern cities of Den Bosch and Boxtel after the animals dug into a dike carrying rails. The national railway company said the line would be out of service for at least a week.\n",
    "The digging means \"the rails can subside and then the safety of train traffic can no longer be guaranteed,\" ProRail, the company that maintains the Dutch rail network said in a statement.\n",
    "Earlier this month, badgers also burrowed under tracks near the northern village of Molkwerum in Friesland province, knocking a line out of service until next month while workers seek permission to shift the animals.\n",
    "Badgers are protected animals in the Netherlands, so rail operators have to get permission to move them or disturb their habitat before repairs can begin.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "747e36f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowed under the tracks of the northern and southern Netherlands, causing delays on the two'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a summary of this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b1ebc5b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrowed under the tracks of the northern and southern Netherlands, causing delays on the two'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Give a very short summary of this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "31ad2e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrow under rail tracks in northern Netherlands, forcing lengthy cancellations on at least two lines'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Write a headline for the following text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e8dfeba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badgers burrow under rail tracks: halt trains in northern and southern Netherlands, forcing lengthy cancellations'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = \"Suggest a headline for this text.\"\n",
    "predict_for_instruction(instruction, text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d5e8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389e506c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
