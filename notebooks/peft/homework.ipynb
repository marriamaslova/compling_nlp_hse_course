{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marriamaslova/compling_nlp_hse_course/blob/master/notebooks/peft/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e14be5b3",
      "metadata": {
        "id": "e14be5b3"
      },
      "source": [
        "## Задание 1.\n",
        "\n",
        "Дообучите языковую модель на датасете инструкций, используя LoRA. Проверьте, что дообученная модель отличается от изначальной - сгенерируйте продолжения для одних и тех же промптов и сравните результаты.\n",
        "\n",
        "Вы можете взять за основу код семинара PEFT, изменив датасет цитат на датасет инструкций (можно просто скопировать из семинара про General_instruct_fine-tuning). \n",
        "Можно использовать alpaca_dataset, датасет Dolly 2 или ваш собственный переведенный датасет (или все вместе). \n",
        "Важно использовать модель с большим количеством параметров (относительно семинара по General instruct fine-tuning). Например, можно попробовать OPT-1.3b,OPT-2.7b или ai-forever/rugpt3large_based_on_gpt2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14a624a4",
      "metadata": {
        "id": "14a624a4"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "1fwd4xiWRgXW"
      },
      "id": "1fwd4xiWRgXW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"facebook/opt-1.3b\", \n",
        "    load_in_8bit=True, \n",
        "    device_map='auto',\n",
        ")"
      ],
      "metadata": {
        "id": "FprE7PviRgVG"
      },
      "id": "FprE7PviRgVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "metadata": {
        "id": "aH1HQtuoRgS5"
      },
      "id": "aH1HQtuoRgS5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in model.parameters():\n",
        "  param.requires_grad = False  # freeze the model - train adapters later\n",
        "  if param.ndim == 1:\n",
        "    # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
        "    param.data = param.data.to(torch.float32)\n",
        "\n",
        "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
        "model.enable_input_require_grads()"
      ],
      "metadata": {
        "id": "Vrp7h2THRgQm"
      },
      "id": "Vrp7h2THRgQm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CastOutputToFloat(nn.Sequential):\n",
        "  def forward(self, x): return super().forward(x).to(torch.float32)\n",
        "model.lm_head = CastOutputToFloat(model.lm_head)"
      ],
      "metadata": {
        "id": "9MKKRJxYRgON"
      },
      "id": "9MKKRJxYRgON",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вспомогательная функция которая покажет сколько параметров будут обучаться\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "4Ij2ZP3LRgL3"
      },
      "id": "4Ij2ZP3LRgL3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model \n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32, \n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "b6sFisCHRgJa"
      },
      "id": "b6sFisCHRgJa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузила датасет инструкций alpaca на Hugging Face, так с большей вероятностью не возникнет проблем с данными. "
      ],
      "metadata": {
        "id": "BXKR9i7C5McU"
      },
      "id": "BXKR9i7C5McU"
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"marriamaslova/instructions_for_peft\")\n",
        "data = data.map(lambda samples: tokenizer(samples['instruction']), batched=True)\n",
        "data = data.map(lambda samples: tokenizer(samples['input']), batched=True)\n",
        "data = data.map(lambda samples: tokenizer(samples['output']), batched=True)\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model, \n",
        "    train_dataset=data['train'],\n",
        "    args=transformers.TrainingArguments(\n",
        "        per_device_train_batch_size=4, \n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=100, \n",
        "        max_steps=400, \n",
        "        learning_rate=2e-4, \n",
        "        fp16=True,\n",
        "        logging_steps=1, \n",
        "        output_dir='outputs'\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")"
      ],
      "metadata": {
        "id": "ckpKeezaRgG3"
      },
      "id": "ckpKeezaRgG3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "0Ho7EeO4UQwb"
      },
      "id": "0Ho7EeO4UQwb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('opt_1.3_lora')"
      ],
      "metadata": {
        "id": "LdNzyLkNa0dA"
      },
      "id": "LdNzyLkNa0dA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# перед запуском этой ячейки нужно перезапустить кернел\n",
        "import torch\n",
        "from peft import PeftModel, PeftConfig\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "peft_model_id = \"opt_1.3_lora\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=\"facebook/opt-1.3b\", \n",
        "                                             return_dict=True, load_in_8bit=True, device_map='auto')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")"
      ],
      "metadata": {
        "id": "AfuwD0h7a46W"
      },
      "id": "AfuwD0h7a46W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(text, tokenizer, model):\n",
        "  batch = tokenizer(text, return_tensors='pt').to('cuda')\n",
        "  output_tokens = model.generate(**batch, max_new_tokens=50, temperature=0.0, no_repeat_ngram_size=2)\n",
        "\n",
        "  return tokenizer.decode(output_tokens[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "b5hW8vOBbPRe"
      },
      "id": "b5hW8vOBbPRe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"I have a dream that\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YFStPoO-bUQB",
        "outputId": "eea503ce-e685-4899-9204-32e034a1f5b9"
      },
      "id": "YFStPoO-bUQB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I have a dream that one day, the world will be a better place.\\nI've got a feeling that's not going to happen.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Star is\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D3Bok-fCdP8u",
        "outputId": "bb4309ee-d9ce-4679-f46f-333b3a57e7de"
      },
      "id": "D3Bok-fCdP8u",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Star is a good guy.\\nHe's a great guy, but he's not a very good actor.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"How can we reduce air pollution?\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "_HdsSd_JfRrH",
        "outputId": "0c29fe87-ccdc-4c3b-88c3-6ab63ab724b8"
      },
      "id": "_HdsSd_JfRrH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How can we reduce air pollution?\\n\\nAir pollution is a major problem in many parts of the world. It is estimated that more than 1.5 billion people worldwide suffer from air-pollution-related diseases.\\nThe World Health Organization (WHO) estimates that air pollutants cause'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"You are in love when\",  tokenizer, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "v9z-tCKrffYS",
        "outputId": "f8438daf-afc9-4633-831b-aeab3ab2f46e"
      },
      "id": "v9z-tCKrffYS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are in love when you are with someone. You are not in a relationship when your heart is not with them.\\nI think this is the best answer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_tuned = PeftModel.from_pretrained(model, peft_model_id)"
      ],
      "metadata": {
        "id": "w8IGgeK9bYcC"
      },
      "id": "w8IGgeK9bYcC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"I have a dream that\",  tokenizer, model_tuned)"
      ],
      "metadata": {
        "id": "ABgvLXULbl5l",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c71904cd-b2d9-482c-f9ad-45de661e1c4d"
      },
      "id": "ABgvLXULbl5l",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have a dream that one day, all people will live in harmony and peace.\\n\\nI believe that we can achieve this dream by working together to create a better world. I believe in the power of people working towards a common goal. Together, we will create the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"Star is\",  tokenizer, model_tuned)"
      ],
      "metadata": {
        "id": "uPDli6wofuyt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "d1ca6209-ae8f-455a-808a-1b6aed892fda"
      },
      "id": "uPDli6wofuyt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Star is a star.\\n\\nThe sun is the sun. \\nIt is bright and shining.\\n\\n\\nStar, star, bright star!\\nYou are the brightest star in the sky.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"How can we reduce air pollution?\",  tokenizer, model_tuned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "xw5LJaLqdmO4",
        "outputId": "48a0bd0a-aa95-403a-ab70-e76611c17562"
      },
      "id": "xw5LJaLqdmO4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'How can we reduce air pollution?\\n\\n1. Reduce the use of fossil fuels.\\n2. Use renewable energy sources. \\n3. Improve public transportation. 4. Increase the efficiency of vehicles. 5. Create more green spaces. 6. Clean up the air. 7'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate(\"You are in love when\",  tokenizer, model_tuned)"
      ],
      "metadata": {
        "id": "64QkLDRefurS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b5aafed7-26f9-448f-81bb-a8f5dfca7069"
      },
      "id": "64QkLDRefurS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You are in love when you feel the warmth of his arms around you. You feel his love for you and you know that you will be together forever.\\n\\nYou feel a sense of peace and contentment when he is by your side. He is your everything and he'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результат понравился, на любой заданный промт дообученная модель дает более интересный ответ"
      ],
      "metadata": {
        "id": "b8IOnJLe5nOb"
      },
      "id": "b8IOnJLe5nOb"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}