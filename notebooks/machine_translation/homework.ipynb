{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marriamaslova/compling_nlp_hse_course/blob/master/notebooks/machine_translation/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dba7c0d",
      "metadata": {
        "id": "1dba7c0d"
      },
      "source": [
        "# Домашнее задание № 11. Машинный перевод"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Yj7aripVIsbG",
      "metadata": {
        "id": "Yj7aripVIsbG"
      },
      "source": [
        "## Задание 1 (6 баллов + 2 доп балла).\n",
        "Нужно обучить трансформер на этом же или на другом корпусе (можно взять другую языковую пару с того же сайте) и оценивать его на всей тестовой выборке (а не на 10 примерах как сделал я). \n",
        "\n",
        "Чтобы получить 2 доп балла вам нужно будет придумать как оптимизировать функцию translate. Подсказка: модель может предсказывать батчами.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05d202c4",
      "metadata": {
        "id": "05d202c4"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers matplotlib sklearn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers "
      ],
      "metadata": {
        "id": "whkDyZBujqX7"
      },
      "id": "whkDyZBujqX7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordPiece\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers import normalizers\n",
        "from tokenizers.normalizers import Lowercase\n",
        "from tokenizers.trainers import WordPieceTrainer\n",
        "from tokenizers import decoders\n",
        "\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import HTML \n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "L5pjQGinjqVm"
      },
      "id": "L5pjQGinjqVm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-train.en"
      ],
      "metadata": {
        "id": "c9hSZ-KajqTG"
      },
      "id": "c9hSZ-KajqTG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = open('opus.en-ru-train.ru').read().replace('\\xa0', ' ')\n",
        "f = open('opus.en-ru-train.ru', 'w')\n",
        "f.write(text)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "68kfAhlyhgoT"
      },
      "id": "68kfAhlyhgoT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents = open('opus.en-ru-train.en').read().lower().splitlines()\n",
        "ru_sents = open('opus.en-ru-train.ru').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "cbkx19oMjqQl"
      },
      "id": "cbkx19oMjqQl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer(WordPiece(), )\n",
        "tokenizer_en.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_en.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_en = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\"])\n",
        "tokenizer_en.train(files=[\"opus.en-ru-train.en\"], trainer=trainer_en )\n",
        "\n",
        "tokenizer_ru = Tokenizer(WordPiece(), )\n",
        "tokenizer_ru.normalizer = normalizers.Sequence([Lowercase()])\n",
        "tokenizer_ru.pre_tokenizer = Whitespace()\n",
        "\n",
        "trainer_ru = WordPieceTrainer(\n",
        "          vocab_size=30000, special_tokens=[\"[UNK]\", \"[PAD]\", \"[START]\", \"[END]\", ])\n",
        "tokenizer_ru.train(files=[\"opus.en-ru-train.ru\"], trainer=trainer_ru )"
      ],
      "metadata": {
        "id": "NO1ysgKFjqOD"
      },
      "id": "NO1ysgKFjqOD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en.decoder = decoders.WordPiece()\n",
        "tokenizer_ru.decoder = decoders.WordPiece()"
      ],
      "metadata": {
        "id": "T-iYxA2ujqLY"
      },
      "id": "T-iYxA2ujqLY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenizer_en.save('tokenizer_en')\n",
        "#tokenizer_ru.save('tokenizer_ru')"
      ],
      "metadata": {
        "id": "s-Nt7pgljqIx"
      },
      "id": "s-Nt7pgljqIx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_en = Tokenizer.from_file(\"tokenizer_en\")\n",
        "tokenizer_ru = Tokenizer.from_file(\"tokenizer_ru\")"
      ],
      "metadata": {
        "id": "JGqgZBvsjqGN"
      },
      "id": "JGqgZBvsjqGN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text, tokenizer, target=False):\n",
        "    if target:\n",
        "        return [tokenizer.token_to_id('[START]')] + tokenizer.encode(text).ids + \\\n",
        "                [tokenizer.token_to_id('[END]')]\n",
        "    else:\n",
        "        return tokenizer.encode(text).ids "
      ],
      "metadata": {
        "id": "_ZCj_Hq3iU7V"
      },
      "id": "_ZCj_Hq3iU7V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = [encode(t, tokenizer_en) for t in en_sents]\n",
        "X_ru = [encode(t, tokenizer_ru, True) for t in ru_sents]"
      ],
      "metadata": {
        "id": "Yis5cUGbiU4e"
      },
      "id": "Yis5cUGbiU4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en = np.max([len(x) for x in X_en])\n",
        "max_len_ru = np.max([len(x) for x in X_ru])"
      ],
      "metadata": {
        "id": "juw8rbK8iU19"
      },
      "id": "juw8rbK8iU19",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len_en, max_len_ru = 45, 48"
      ],
      "metadata": {
        "id": "mRWMY765iUzA"
      },
      "id": "mRWMY765iUzA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PAD_IDX = tokenizer_ru.token_to_id('[PAD]')"
      ],
      "metadata": {
        "id": "jYq0XrFGiUwW"
      },
      "id": "jYq0XrFGiUwW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_en = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              X_en, maxlen=max_len_en, padding='post', value=PAD_IDX)\n",
        "\n",
        "X_ru_out = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[1:] for x in X_ru], maxlen=max_len_ru-1, padding='post', \n",
        "              value=PAD_IDX)\n",
        "\n",
        "X_ru_dec = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "              [x[:-1] for x in X_ru], maxlen=max_len_ru-1, \n",
        "              padding='post', value=PAD_IDX)"
      ],
      "metadata": {
        "id": "69VtmhN6jp-m"
      },
      "id": "69VtmhN6jp-m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_en_train, X_en_valid, \n",
        "X_ru_dec_train, X_ru_dec_valid, \n",
        "X_ru_out_train, X_ru_out_valid) = train_test_split(X_en, \n",
        "                                                  X_ru_dec, \n",
        "                                                  X_ru_out, \n",
        "                                                  test_size=0.05)"
      ],
      "metadata": {
        "id": "sqCBm8YgmV0U"
      },
      "id": "sqCBm8YgmV0U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "R8seeFiCmdfJ"
      },
      "id": "R8seeFiCmdfJ"
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "    # Считаем скалярное произведение между запросом (query) и ключом (key), транспонируя ключ\n",
        "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "    # Получаем глубину (размерность) ключа и преобразуем ее во float\n",
        "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "\n",
        "    # Делим результат скалярного произведения на квадратный корень из глубины\n",
        "    # Это делается для уменьшения влияния больших значений и стабилизации градиентов во время обучения\n",
        "    logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "    # Если есть маска, применяем ее к логитам, чтобы обнулить нежелательные значения\n",
        "    if mask is not None:\n",
        "        logits += (mask * -1e9)\n",
        "\n",
        "    # Применяем функцию softmax для получения весов внимания\n",
        "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "    # Умножаем веса внимания на значения (value) для получения итогового результата\n",
        "    output = tf.matmul(attention_weights, value)\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "s6DPpBgFjhke"
      },
      "id": "s6DPpBgFjhke",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "        super(MultiHeadAttention, self).__init__(name=name)\n",
        "        self.num_heads = num_heads  # количество голов для внимания\n",
        "        self.d_model = d_model  # размерность вектора модели\n",
        "\n",
        "        # Убеждаемся, что размерность модели делится нацело на количество голов\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads  # размерность каждой головы\n",
        "\n",
        "        # Создаем полносвязные слои для запроса, ключа и значения\n",
        "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "        # Создаем последний полносвязный слой\n",
        "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        # Разделяем входные данные на головы\n",
        "        inputs = tf.reshape(\n",
        "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
        "            'value'], inputs['mask']\n",
        "        batch_size = tf.shape(query)[0]\n",
        "\n",
        "        # Пропускаем запрос, ключ и значение через соответствующие полносвязные слои\n",
        "        query = self.query_dense(query)\n",
        "        key = self.key_dense(key)\n",
        "        value = self.value_dense(value)\n",
        "\n",
        "        # Разделяем запрос, ключ и значение на головы \n",
        "        # то есть просто разрезаем вектора на num_heads частей \n",
        "        # и сравниваем все части между собой\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        # Выполняем механизм внимания с масштабированным скалярным произведением\n",
        "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        # Объединяем головы вместе (склеиваем векторы в один)\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))\n",
        "\n",
        "        # Пропускаем объединенное внимание через дополнительный полносвязный слой\n",
        "        # Он просто добавляет сложности нашей модели\n",
        "        outputs = self.dense(concat_attention)\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "gCbZNTvOjhiD"
      },
      "id": "gCbZNTvOjhiD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(x):\n",
        "    mask = tf.cast(tf.math.equal(x, PAD_IDX), tf.float32)\n",
        "    # (batch_size, 1, 1, sequence length)\n",
        "    return mask[:, tf.newaxis, tf.newaxis, :]"
      ],
      "metadata": {
        "id": "6LeMA4Bujhfi"
      },
      "id": "6LeMA4Bujhfi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_look_ahead_mask(x):\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    padding_mask = create_padding_mask(x)\n",
        "    return tf.maximum(look_ahead_mask, padding_mask)"
      ],
      "metadata": {
        "id": "p0xnEkU1jhdA"
      },
      "id": "p0xnEkU1jhdA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, position, d_model):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
        "\n",
        "    def get_angles(self, position, i, d_model):\n",
        "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return position * angles\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "        d_model=d_model)\n",
        "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
        "\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
        "        return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
      ],
      "metadata": {
        "id": "-PKUBNhFjhaa"
      },
      "id": "-PKUBNhFjhaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    #call_mha\n",
        "    attention = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention\")({\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
        "    attention = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(inputs + attention)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention + outputs)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "uXnEiHj4jhX2"
      },
      "id": "uXnEiHj4jhX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name=\"encoder\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    #inputs (они тут называются outputs но это просто такой нейминг, \n",
        "    # этот параметр передается в encoder_layer первым и encoder_layer будет считать его inputs)\n",
        "    # outputs он тут называется для удобства, так как он будет перезаписываться\n",
        "    # чтобы на вход следующему блоку подавать уже не эмбединги,\n",
        "    # а то что получится как результат предыдущего блока\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = encoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name=\"encoder_layer_{}\".format(i),\n",
        "        )([outputs, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, padding_mask], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "0k-87L80jhU5"
      },
      "id": "0k-87L80jhU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
        "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    attention1 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
        "          'query': inputs,\n",
        "          'key': inputs,\n",
        "          'value': inputs,\n",
        "          'mask': look_ahead_mask\n",
        "      })\n",
        "    attention1 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention1 + inputs)\n",
        "\n",
        "    attention2 = MultiHeadAttention(\n",
        "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
        "          'query': attention1,\n",
        "          'key': enc_outputs,\n",
        "          'value': enc_outputs,\n",
        "          'mask': padding_mask\n",
        "      })\n",
        "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
        "    attention2 = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(attention2 + attention1)\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
        "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
        "    outputs = tf.keras.layers.LayerNormalization(\n",
        "      epsilon=1e-6)(outputs + attention2)\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "70bsCECNnATI"
      },
      "id": "70bsCECNnATI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder(vocab_size,\n",
        "            num_layers,\n",
        "            units,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            dropout,\n",
        "            max_len,\n",
        "            name='decoder'):\n",
        "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
        "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
        "    look_ahead_mask = tf.keras.Input(\n",
        "      shape=(1, None, None), name='look_ahead_mask')\n",
        "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
        "\n",
        "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
        "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
        "    embeddings = PositionalEncoding(max_len, d_model)(embeddings)\n",
        "\n",
        "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
        "\n",
        "    for i in range(num_layers):\n",
        "        outputs = decoder_layer(\n",
        "            units=units,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            name='decoder_layer_{}'.format(i),\n",
        "        )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
        "\n",
        "    return tf.keras.Model(\n",
        "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
        "      outputs=outputs,\n",
        "      name=name)"
      ],
      "metadata": {
        "id": "vK1h89QZnAQn"
      },
      "id": "vK1h89QZnAQn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transformer(vocab_size,\n",
        "                num_layers,\n",
        "                units,\n",
        "                d_model,\n",
        "                num_heads,\n",
        "                dropout,\n",
        "                max_len,\n",
        "                name=\"transformer\"):\n",
        "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
        "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
        "\n",
        "    enc_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='enc_padding_mask')(inputs)\n",
        "    \n",
        "    look_ahead_mask = tf.keras.layers.Lambda(\n",
        "      create_look_ahead_mask,\n",
        "      output_shape=(1, None, None),\n",
        "      name='look_ahead_mask')(dec_inputs)\n",
        "    \n",
        "    dec_padding_mask = tf.keras.layers.Lambda(\n",
        "      create_padding_mask, output_shape=(1, 1, None),\n",
        "      name='dec_padding_mask')(inputs)\n",
        "\n",
        "    enc_outputs = encoder(\n",
        "                          vocab_size=vocab_size[0],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[0],\n",
        "                        )(inputs=[inputs, enc_padding_mask])\n",
        "\n",
        "    dec_outputs = decoder(\n",
        "                          vocab_size=vocab_size[1],\n",
        "                          num_layers=num_layers,\n",
        "                          units=units,\n",
        "                          d_model=d_model,\n",
        "                          num_heads=num_heads,\n",
        "                          dropout=dropout,\n",
        "                          max_len=max_len[1],\n",
        "                        )(inputs=[dec_inputs, enc_outputs, \n",
        "                                  look_ahead_mask, dec_padding_mask])\n",
        "\n",
        "    outputs = tf.keras.layers.Dense(units=vocab_size[1], name=\"outputs\")(dec_outputs)\n",
        "\n",
        "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
      ],
      "metadata": {
        "id": "-4cV05bWnAOK"
      },
      "id": "-4cV05bWnAOK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "L  = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "      from_logits=True, reduction='none',)\n",
        "\n",
        "def loss_function(y_true, y_pred):\n",
        "    loss = L(y_true, y_pred)\n",
        "\n",
        "    mask = tf.cast(tf.not_equal(y_true, PAD_IDX), tf.float32)\n",
        "    loss = tf.multiply(loss, mask)\n",
        "\n",
        "    return tf.reduce_mean(loss)"
      ],
      "metadata": {
        "id": "KaeaNPz2jp2z"
      },
      "id": "KaeaNPz2jp2z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# small model\n",
        "NUM_LAYERS = 2\n",
        "D_MODEL = 256\n",
        "NUM_HEADS = 8\n",
        "\n",
        "UNITS = 512\n",
        "DROPOUT = 0.1\n",
        "\n",
        "\n",
        "# average model\n",
        "# NUM_LAYERS = 6\n",
        "# D_MODEL = 512\n",
        "# NUM_HEADS = 8\n",
        "# UNITS = 2048\n",
        "# DROPOUT = 0.1\n",
        "\n",
        "\n",
        "model = transformer(\n",
        "    vocab_size=(tokenizer_en.get_vocab_size(),tokenizer_ru.get_vocab_size()),\n",
        "    num_layers=NUM_LAYERS,\n",
        "    units=UNITS,\n",
        "    d_model=D_MODEL,\n",
        "    num_heads=NUM_HEADS,\n",
        "    dropout=DROPOUT,\n",
        "    max_len=[max_len_en, max_len_ru])\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    0.001, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
        "\n",
        "def accuracy(y_true, y_pred):\n",
        "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('model_ruen',\n",
        "                                            monitor='val_loss',\n",
        "                                            verbose=1,\n",
        "                                            save_weights_only=True,\n",
        "                                            save_best_only=True,\n",
        "                                            mode='min',\n",
        "                                            save_freq='epoch')"
      ],
      "metadata": {
        "id": "F5E1PYd8kIgA"
      },
      "id": "F5E1PYd8kIgA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit((X_en_train, X_ru_dec_train), X_ru_out_train, \n",
        "             validation_data=((X_en_valid, X_ru_dec_valid), X_ru_out_valid),\n",
        "             batch_size=500,\n",
        "             epochs=1,\n",
        "             callbacks=[checkpoint]\n",
        "             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDck_wHckIWb",
        "outputId": "3df146b3-0162-44a5-9703-8b328d65d079"
      },
      "id": "cDck_wHckIWb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1900/1900 [==============================] - ETA: 0s - loss: 1.3720 - accuracy: 0.1090\n",
            "Epoch 1: val_loss improved from inf to 0.97220, saving model to model_ruen\n",
            "1900/1900 [==============================] - 1602s 833ms/step - loss: 1.3720 - accuracy: 0.1090 - val_loss: 0.9722 - val_accuracy: 0.1464\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8894e18e20>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(text):\n",
        "    input_ids = encode(text, tokenizer_en, target=False)\n",
        "\n",
        "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "                                      [input_ids], maxlen=max_len_en, padding='post', \n",
        "                                       # важно не забыть паддинг с нужным id\n",
        "                                       value=PAD_IDX)\n",
        "\n",
        "    \n",
        "    \n",
        "    output_ids = [tokenizer_ru.token_to_id('[START]') ]\n",
        "    \n",
        "    pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
        " \n",
        "    \n",
        "    while pred.argmax(2)[0][-1] not in [tokenizer_ru.token_to_id('[END]')]:\n",
        "        if len(output_ids) >= max_len_ru:\n",
        "            break\n",
        "        # можно занизить скор тэга UNK чтобы он никогда не генерировался\n",
        "        pred[:, :, tokenizer_ru.token_to_id('[UNK]')] = -100\n",
        "\n",
        "        output_ids.append(pred.argmax(2)[0][-1])\n",
        "        pred = model((input_ids, tf.cast([output_ids], tf.int32)), training=False).numpy()\n",
        "\n",
        "    return tokenizer_ru.decode(output_ids[1:], )"
      ],
      "metadata": {
        "id": "Nq-BPn73kII9"
      },
      "id": "Nq-BPn73kII9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install nltk"
      ],
      "metadata": {
        "id": "_GOmvtdXkH5s"
      },
      "id": "_GOmvtdXkH5s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "hypothesis = ['It', 'is', 'a', 'cat', 'at', 'room']\n",
        "reference = ['It', 'is', 'a', 'cat', 'inside', 'the', 'room']\n",
        "\n",
        "\n",
        "BLEUscore = nltk.translate.bleu_score.sentence_bleu([reference], hypothesis, auto_reweigh=True)\n",
        "print(BLEUscore)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YClfKINskH15",
        "outputId": "e807373d-49c8-4ca8-9fa7-88fc4e7e1a45"
      },
      "id": "YClfKINskH15",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4548019047027907\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.ru\n",
        "!wget https://data.statmt.org/opus-100-corpus/v1.0/supervised/en-ru/opus.en-ru-test.en"
      ],
      "metadata": {
        "id": "Vjs5voskwjhG"
      },
      "id": "Vjs5voskwjhG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "en_sents_test = open('opus.en-ru-test.en').read().lower().splitlines()\n",
        "ru_sents_test = open('opus.en-ru-test.ru').read().lower().splitlines()"
      ],
      "metadata": {
        "id": "rtX7uwaskHwj"
      },
      "id": "rtX7uwaskHwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translations = []\n",
        "\n",
        "for i in range(len(en_sents_test)):\n",
        "    translations.append(translate(en_sents_test[i]))"
      ],
      "metadata": {
        "id": "4IKE_ryTjpz-"
      },
      "id": "4IKE_ryTjpz-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleus = []\n",
        "\n",
        "for i, t in enumerate(translations):\n",
        "    reference = tokenizer_ru.encode(t).tokens\n",
        "    hypothesis = tokenizer_ru.encode(ru_sents_test[i]).tokens\n",
        "\n",
        "    bleus.append(nltk.translate.bleu_score.sentence_bleu([reference], hypothesis,  auto_reweigh=True))"
      ],
      "metadata": {
        "id": "w2iL4aOtjpxP"
      },
      "id": "w2iL4aOtjpxP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(sum(bleus)/len(bleus))*100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ut92fwBT9Rlw",
        "outputId": "f995534e-ee77-457d-8c79-00756bbe94fe"
      },
      "id": "Ut92fwBT9Rlw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.49825966067874"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa93d6",
      "metadata": {
        "id": "b5aa93d6"
      },
      "source": [
        "\n",
        "## Задание 2 (2 балла).\n",
        "Прочитайте главу про машинный перевод у Журафски и Маннига - https://web.stanford.edu/~jurafsky/slp3/10.pdf \n",
        "Ответьте своими словами в чем заключается техника back translation? Для чего она применяется и что позволяет получить? Опишите по шагам как его применить к паре en-ru на данных из семинара. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backtranslation** - один из методов, относящихся к технике Data augmentation (увеличения данных), которая применяется при недостатке обучающих данных (часто при работе с малоресурсными языками). \n",
        "Идея обратного перевода опирается на предположение, что существующие небольшие параллельные корпуса можно дополнить синтетическими битекстами, основанными на материалах одноязычного корпуса. \n",
        "Предположим, у нас есть небольшой параллельный корпус en-ru, где en - исходный язык, ru - целевой, а также много данных на ru. При обратном переводе модель сначала обучается на существующем параллельном корпусе в обратную сторону - перевод «ЦЯ-ИЯ». Эту модель используется для перевода одноязычных данных ru на en. Таким образом мы получаем синтетический параллельный корпус, данные которого можем перемешать с изначальным параллельным корпусом и затем обучить модель на всех имеющихся данных уже в направлении «ИЯ-ЦЯ»."
      ],
      "metadata": {
        "id": "TX_2uc-l_Un2"
      },
      "id": "TX_2uc-l_Un2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}