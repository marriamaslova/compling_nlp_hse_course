{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marriamaslova/compling_nlp_hse_course/blob/master/notebooks/spelling/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "371970ff",
      "metadata": {
        "id": "371970ff"
      },
      "source": [
        "# Домашнее задание № 3. Исправление опечаток"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35cf8bd",
      "metadata": {
        "id": "b35cf8bd"
      },
      "source": [
        "## 1. Доп. ранжирование по вероятности (3 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c6be25c",
      "metadata": {
        "id": "0c6be25c"
      },
      "source": [
        "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ответ с демонстрацией результата"
      ],
      "metadata": {
        "id": "jW_GG_QEoQe_"
      },
      "id": "jW_GG_QEoQe_"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install razdel tqdm"
      ],
      "metadata": {
        "id": "p5PreWLXoG95"
      },
      "id": "p5PreWLXoG95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re\n",
        "from string import punctuation\n",
        "import numpy as np\n",
        "import json\n",
        "from collections import Counter\n",
        "from pprint import pprint\n",
        "from nltk import sent_tokenize\n",
        "punctuation += \"«»—…“”\"\n",
        "punct = set(punctuation)\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from string import punctuation\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "Qkb_XpMBoG7r"
      },
      "id": "Qkb_XpMBoG7r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "KMNN2b7coG5Q"
      },
      "id": "KMNN2b7coG5Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textdistance"
      ],
      "metadata": {
        "id": "mFW5kjvEoG3K"
      },
      "id": "mFW5kjvEoG3K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textdistance"
      ],
      "metadata": {
        "id": "jrz4KCg-oG1A"
      },
      "id": "jrz4KCg-oG1A",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from difflib import get_close_matches"
      ],
      "metadata": {
        "id": "M4NXlhxeoGyp"
      },
      "id": "M4NXlhxeoGyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances"
      ],
      "metadata": {
        "id": "viFTrH3KoGwS"
      },
      "id": "viFTrH3KoGwS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter, attrgetter, methodcaller"
      ],
      "metadata": {
        "id": "H7oO829cpAEi"
      },
      "id": "H7oO829cpAEi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = open('wiki_data.txt', encoding='utf8').read()"
      ],
      "metadata": {
        "id": "5_Zll4n9oGtr"
      },
      "id": "5_Zll4n9oGtr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter(re.findall('\\w+', corpus.lower()))\n",
        "\n",
        "word2id = list(vocab.keys())\n",
        "id2word = {i:word for i, word in enumerate(vocab)}\n",
        "\n",
        "\n",
        "vec = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
        "X = vec.fit_transform(vocab)"
      ],
      "metadata": {
        "id": "AStbXv7NoGrQ"
      },
      "id": "AStbXv7NoGrQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_closest_match_with_metric(text, lookup,topn=20, metric=textdistance.levenshtein):\n",
        "    # Counter можно использовать и с не целыми числами\n",
        "    similarities = Counter()\n",
        "    \n",
        "    for word in lookup:\n",
        "        similarities[word] = metric.normalized_similarity(text, word) \n",
        "    \n",
        "    return similarities.most_common(topn)\n",
        "\n",
        "def keyFunc(elem):\n",
        "    return elem[2]\n",
        "\n",
        "N = sum(vocab.values())\n",
        "\n",
        "def P(word, N=N):\n",
        "    return vocab[word] / N\n",
        "\n",
        "def predict_mistaken(word, vocab):\n",
        "    return 0 if word in vocab else 1"
      ],
      "metadata": {
        "id": "nPy7j5WCoGmP"
      },
      "id": "nPy7j5WCoGmP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_closest_match_vec(text, X, vec, topn=20):\n",
        "    v = vec.transform([text])\n",
        "    \n",
        "    # вся эффективноть берется из того, что мы сразу считаем близость \n",
        "    # 1 вектора ко всей матрице (словам в словаре)\n",
        "    # считать по отдельности циклом было бы дольше\n",
        "    # вместо одного вектора может даже целая матрица\n",
        "    # тогда считаться в итоге будет ещё быстрее\n",
        "    \n",
        "    similarities = cosine_distances(v, X)[0]\n",
        "    topn = similarities.argsort()[:topn] \n",
        "    \n",
        "    return [(id2word[top], similarities[top]) for top in topn]"
      ],
      "metadata": {
        "id": "bmuklrXro0wE"
      },
      "id": "bmuklrXro0wE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_closest_hybrid_match(text, X, vec, topn=3, metric=textdistance.damerau_levenshtein):\n",
        "    candidates = get_closest_match_vec(text, X, vec, topn*4)\n",
        "    lookup = [cand[0] for cand in candidates]\n",
        "    closest = get_closest_match_with_metric(text, lookup, topn, metric=metric)\n",
        "    elems = []\n",
        "    for elem in closest:\n",
        "      elem_сh = list(elem)\n",
        "      elems.append(elem_сh)\n",
        "    for elem in elems:\n",
        "      for el in elem:\n",
        "        if elem.index(el) == 0:\n",
        "          elem.append(P(el))\n",
        "    result = sorted(elems, key=itemgetter(2), reverse=True)\n",
        "    closest = result[0]\n",
        "    return closest"
      ],
      "metadata": {
        "id": "LSv39Z5eo0ta"
      },
      "id": "LSv39Z5eo0ta",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_closest_hybrid_match('крйг', X, vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYNg0N6Po0rJ",
        "outputId": "148a0f85-14d9-455e-d46a-cc7d4b743140"
      },
      "id": "WYNg0N6Po0rJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['крейг', 0.8, 3.491566605803488e-06]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9cf9985",
      "metadata": {
        "id": "f9cf9985"
      },
      "source": [
        "## 2.  Symspell (5 баллов)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9392cc23",
      "metadata": {
        "id": "9392cc23"
      },
      "source": [
        "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Там к словам применяется только одна операция - удаление символа. Описание алгоритма по шагам:\n",
        "\n",
        "1) Составляется словарь правильных слов  \n",
        "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово   \n",
        "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
        "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
        "\n",
        "\n",
        "Оцените качество полученного алгоритма теми же тремя метриками."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем словарь правильных слов"
      ],
      "metadata": {
        "id": "6s11vd_wrjMd"
      },
      "id": "6s11vd_wrjMd"
    },
    {
      "cell_type": "code",
      "source": [
        "bad = open('sents_with_mistakes.txt', encoding='utf8').read().splitlines()\n",
        "true = open('correct_sents.txt', encoding='utf8').read().splitlines()"
      ],
      "metadata": {
        "id": "h0JrdKYv26AH"
      },
      "id": "h0JrdKYv26AH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def align_words(sent_1, sent_2):\n",
        "    tokens_1 = sent_1.lower().split()\n",
        "    tokens_2 = sent_2.lower().split()\n",
        "    \n",
        "    tokens_1 = [token.strip(punctuation) for token in tokens_1]\n",
        "    tokens_2 = [token.strip(punctuation) for token in tokens_2]\n",
        "    \n",
        "    tokens_1 = [token for token in tokens_1 if token]\n",
        "    tokens_2 = [token for token in tokens_2 if token]\n",
        "    \n",
        "    assert len(tokens_1) == len(tokens_2)\n",
        "    \n",
        "    return list(zip(tokens_1, tokens_2))"
      ],
      "metadata": {
        "id": "W6HWe0Fl3GVl"
      },
      "id": "W6HWe0Fl3GVl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем словарь правильных слов"
      ],
      "metadata": {
        "id": "3za-SflNib6s"
      },
      "id": "3za-SflNib6s"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_l = list(vocab)"
      ],
      "metadata": {
        "id": "XyMbrkyXrWRz"
      },
      "id": "XyMbrkyXrWRz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем словарь удалений\n"
      ],
      "metadata": {
        "id": "zwXwFjKtroeL"
      },
      "id": "zwXwFjKtroeL"
    },
    {
      "cell_type": "code",
      "source": [
        "def edits(word):\n",
        "  letters    = 'йцукенгшщзхъфывапролджэячсмитьбюё'\n",
        "  splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "  deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "  return(deletes)"
      ],
      "metadata": {
        "id": "e9pjI1ZprWP8"
      },
      "id": "e9pjI1ZprWP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edits2(word): \n",
        "    return list(e2 for e1 in edits(word) for e2 in edits(e1))"
      ],
      "metadata": {
        "id": "wL6Xc7_5wssB"
      },
      "id": "wL6Xc7_5wssB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "changes = []\n",
        "for word in vocab:\n",
        "  changes.append(edits(word) + edits2(word))"
      ],
      "metadata": {
        "id": "bt9We9d-rWNj"
      },
      "id": "bt9We9d-rWNj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ch_dict = []\n",
        "for change, word in zip(changes, vocab_l):\n",
        "  ch_dict.append(dict.fromkeys(change, word))"
      ],
      "metadata": {
        "id": "sGlC3l5RrWLo"
      },
      "id": "sGlC3l5RrWLo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Генерируем варианты ошибок и ищем исправление"
      ],
      "metadata": {
        "id": "1R0kp86KvUh4"
      },
      "id": "1R0kp86KvUh4"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(word):\n",
        "  options = edits(word) + edits2(word)\n",
        "  candidates = []\n",
        "  for elem in ch_dict:\n",
        "    for opt, key in zip(options, elem):\n",
        "      if opt == key:\n",
        "        candidates.append(elem[key])\n",
        "      elif opt == elem[key]:\n",
        "        candidates.append(elem[key])\n",
        "      else:\n",
        "        continue\n",
        "  almost_answer = []\n",
        "  for cand in candidates:\n",
        "    cand = [cand, P(cand)]\n",
        "    almost_answer.append(cand)\n",
        "  answer = sorted(almost_answer, key=itemgetter(1), reverse=True)\n",
        "  return answer[0]"
      ],
      "metadata": {
        "id": "zLitxCkI1_zr"
      },
      "id": "zLitxCkI1_zr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer('сонце')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOUSFZrW2fU2",
        "outputId": "9f7a69c9-3f7a-4554-d2ca-6d8574a046a0"
      },
      "id": "ZOUSFZrW2fU2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['конце', 0.00037068798798280367]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_answer('чаащ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQJeethXokuO",
        "outputId": "f4add6fb-385a-4dae-d634-ac38ff3b977f"
      },
      "id": "bQJeethXokuO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['чаще', 8.030603193348022e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b292d96d",
      "metadata": {
        "id": "b292d96d"
      },
      "source": [
        "## *3. Чтение. (2 балла)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dee4b28f",
      "metadata": {
        "id": "dee4b28f"
      },
      "source": [
        "Прочитайте эту главу в книге Speech and Language Processing - https://web.stanford.edu/~jurafsky/slp3/2.pdf .\n",
        "Ответьте на следующий вопрос:\n",
        "\n",
        "1. Что такое Byte-Pair Encoding (опишите по-русски, как минимум 10 предложениями)?\n",
        "\n",
        "*Это задание не связано напрямую с исправлением опечаток, но это важная тема, к которой мы вернемся в конце курса"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Byte-Pair Encoding - одна из опций для токенизации текста. В таком случае при токенизации опираются не на какое-то правило, а на соственно статистику по данным. Это особенно полезно в случае, если при обработке встречаются несловарные слова. \n",
        "\n",
        "Начнем с того, что при создании токенизатора рассматриваются не только слова, но и \"подслова\", в основном, продуктивные аффиксы. Таким образом, несловарное слово может быть представлено как последовательность подслов. \n",
        "\n",
        "Работа алгоритма byte-pair encoding начанается с того, что на основании текстового корпуса выясняется, какие элементы из словаря отдельных символов чаще всего встречаются вместе. После этого все соседствующие условные A и B заменяются на AB. И таким образом последовательности постеменно удлиняются (до k элементов (k устанавливается в алгоритме)). Результирующий словарь состоит из исходного набора символов плюс k новых символов. \n",
        "\n",
        "После обучения алгоритм запускается на тестовых данных и работает по подобному принципу, однако уже опираясь на информацию об устойчивых последовательностях из обучающих данных. Если же попадается неизвестное слово, то, скорее всего, оно будет представлено как несколько частей, внутри которых была статистически обнаружена связь. "
      ],
      "metadata": {
        "id": "hqamyiIB58mi"
      },
      "id": "hqamyiIB58mi"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}